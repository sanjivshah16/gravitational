Article

Multi-timescale reinforcement learning in
the brain

https://doi.org/10.1038/s41586-025-08929-9                        Paul Masset1,2,3,4,11 ✉, Pablo Tano5,11, HyungGoo R. Kim1,2,6,7, Athar N. Malik1,2,8,9,
                                                                  Alexandre Pouget5 ✉ & Naoshige Uchida1,2,10 ✉
Received: 12 November 2023

Accepted: 21 March 2025
                                                                  To thrive in complex environments, animals and artificial agents must learn to act
Published online: xx xx xxxx
                                                                  adaptively to maximize fitness and rewards. Such adaptive behaviour can be learned
    Check for updates
                                                                  through reinforcement learning1, a class of algorithms that has been successful at
                                                                  training artificial agents2–5 and at characterizing the firing of dopaminergic neurons in
                                                                  the midbrain6–8. In classical reinforcement learning, agents discount future rewards
                                                                  exponentially according to a single timescale, known as the discount factor. Here we
                                                                  explore the presence of multiple timescales in biological reinforcement learning.
                                                                  We first show that reinforcement agents learning at a multitude of timescales possess
                                                                  distinct computational benefits. Next, we report that dopaminergic neurons in mice
                                                                  performing two behavioural tasks encode reward prediction error with a diversity
                                                                  of discount time constants. Our model explains the heterogeneity of temporal
                                                                  discounting in both cue-evoked transient responses and slower timescale fluctuations
                                                                  known as dopamine ramps. Crucially, the measured discount factor of individual
                                                                  neurons is correlated across the two tasks, suggesting that it is a cell-specific property.
                                                                  Together, our results provide a new paradigm for understanding functional
                                                                  heterogeneity in dopaminergic neurons and a mechanistic basis for the empirical
                                                                  observation that humans and animals use non-exponential discounts in many
                                                                  situations9–12, and open new avenues for the design of more-efficient reinforcement
                                                                  learning algorithms.


Many of the recent advances in artificial intelligence rely on temporal                                The TD rule can potentially be extended to learn more complex
difference (TD) reinforcement learning (RL) in which the TD learning                                predictive representations than the mean discounted future reward
rule is used to learn predictive information1 (equation (2)). By updat-                             of the traditional value function, in both artificial neural systems16–19
ing current estimates on the basis of future expected estimates, TD                                 and biological neural systems20–24. A growing body of evidence points
methods have been remarkably successful in solving tasks that require                               to the rich nature of temporal representations in biological systems25,26
predicting future rewards and planning actions to obtain them2,13.                                  and particularly in the basal ganglia27–31. Understanding how these
   The standard formulation of TD learning assumes a fixed discount                                 rich temporal representations are learned remains a key question
factor (that is, a single-learning timescale), which, after convergence,                            in neuroscience and psychology. An important component across
results in exponential discounting: the value of a future reward is                                 most temporal-learning proposals is the presence of multiple time-
reduced by a fixed fraction per unit time (or timestep). Although                                   scales22,31–35, which enables capturing temporal dependencies across
this formulation is important for simplicity and self-consistency of                                a diverse range of durations: shorter timescales typically handle rapid
the learning rule, it is well known that humans and other animals do                                changes and immediate dependencies, whereas longer timescales
not exhibit exponential discounting when faced with inter-temporal                                  capture slow-changing features or long-term dependencies34. Further-
choices. Instead, they tend to show hyperbolic discounting: there is a                              more, work in artificial intelligence has suggested that the performance
fast drop in value followed by a slower rate for further delays9,10. Far from                       of deep RL algorithms can be improved by incorporating learning at
being irrational, non-exponential discounting can be optimal depend-                                multiple timescales19,36. We therefore ask whether RL in the brain exhib-
ing on the uncertainty in the environment, as has been documented                                   its such multi-timescale properties.
in the behavioural economics and foraging literature11,12. Humans and                                  We first investigate the computational implications of multi-
animals can modulate their discounting function to adapt to the tem-                                timescale RL. We then show that dopaminergic neurons encode predic-
poral statistics of the environment and maladaptive behaviour can be                                tions at diverse timescales, providing a potential neural substrate for
a signature of mental state or disease14,15.                                                        multi-timescale RL in the brain.

Department of Molecular and Cellular Biology, Harvard University, Cambridge, MA, USA. 2Center for Brain Science, Harvard University, Cambridge, MA, USA. 3Department of Psychology,
1


McGill University, Montréal, Québec, Canada. 4Mila – Quebec Artificial Intelligence Institute, Montréal, Québec, Canada. 5Department of Basic Neuroscience, Université de Genève, Geneva,
Switzerland. 6Department of Biomedical Engineering, Sungkyunkwan University, Suwon, Republic of Korea. 7Center for Neuroscience Imaging Research, Institute for Basic Science (IBS),
Suwon, Republic of Korea. 8Department of Neurosurgery, Warren Alpert Medical School of Brown University, Providence, RI, USA. 9Norman Prince Neurosciences Institute, Rhode Island Hospital,
Providence, RI, USA. 10Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA, USA. 11These authors contributed equally: Paul Masset, Pablo Tano.
✉e-mail: paul.masset@mcgill.ca; alexandre.pouget@unige.ch; uchida@mcb.harvard.edu



                                                                                                                                                          Nature | www.nature.com | 1
Article
a                                                                                     b
     Reward timing                           Discounting                                  Reward timing                          Discounting                            Value space                 L–1   Temporal space

                                                                                                                                                                                                    L



                          Value of cue




                                                                   Value of cue




                                                                                                            Value of cue




                                                                                                                                                     Value of cue
    0                                    0                                                0                                0                                        0                          1          0
          Time                                   Time                                          Time                                  Time                               Discount factor                       Time

c                                            s                   tR                   e                                                                                            f
       Episode
                      States                                                                         Task 1                                 Task 2
     (defined by
                                                                                                                                                                                               1
      tR and R)       Reward                 0    0         0    R                0           Report reward time tR                   Report value of s
                                                                                                independently of                    based on a hyperbolic
d                                                                                             reward magnitude R                          discount




                                                                                                                                                                                   Accuracy
           Step 1                                       Step 2
        Learn values                         Use values to produce report                                                                                                                     0.5
     (for each episode)                       (trained across episodes)
                                                 V1 (s)                                             Task 3                                Task 4
           V1 (s)
                                                                                              Report reward time tR                   Report value of s
           V2 (s)                               V2 (s)               Report                  before TD learning                     based on a single                                        0
           V3 (s)                                                                                 converges                           noisy trajectory                                             1       2        3
                                                 V3 (s)
                                                                                                                                                                                                    Number of discounts
        TD learning                                   Policy gradient


Fig. 1 | Computational advantages of multi-timescale RL. a, In single-                                   decoder to maximize the accuracy of a task-specific report. The decoder is
timescale value learning, the value of a cue (at t = 0) predicting future rewards                        trained across episodes using policy gradient. e, The architecture is trained
(left) is evaluated by discounting these rewards with a single exponential                               across four tasks to highlight computational advantages of multi-timescale
discounting function (middle). The expected reward size and timing are                                   RL, including decoupling information about reward size and reward timing,
encoded, but confounded, in the value of the cue (right). b, In multi-timescale                          the ability to learn with arbitrary discount functions, the ability to recover
value learning, the same reward delays are evaluated with multiple discounting                           reward timing information before convergence and the ability to control
functions (middle left). The relative value of a cue as a function of the discount                       the inductive bias (see main text and Methods). f, Mean accuracy is reported
depends on the reward delay (middle right). A simple linear decoder based on                             after 2,000 training episodes as the fraction of correct responses (see Methods).
the Laplace transform can thus reconstruct both the expected timing and the                              ‘Three discounts’ correspond to the γ-ensemble [0.6,0.9,0.99], ‘one discount’
magnitude of rewards (right). c, Experiment to compare single-timescale                                  to the top-performing ensemble across {[0.6,0.6,0.6], [0.9,0.9,0.9],
versus multi-timescale learning. t R and R are fixed within each episode and                             [0.99,0.99,0.99]} and analogous for ‘two discounts’. The error bars are the
varied across episodes. d, In each episode (defined by a specific t R and R),                            standard deviations (s.d.) across 100 test episodes and 3 trained policy
the value function is learned via tabular updates, using multiple discount                               gradient networks.
factors (step 1). Given these values, step 2 consists of training a non-linear


                                                                                                            Now consider multi-timescale learning (Fig. 1b). Let Vi be the value
Advantages of multi-timescale RL                                                                         learned using a discount γi. Moving the discount factor γ out of the
We first examined the computational advantages of RL agents using                                        expectation in equation (1), values can be rewritten (truncating at
multiple timescales over those utilizing a single timescale. We start                                    t = T) as
with a simple example environment in which a cue (s) predicts a future                                                                                E (r|t = 0) 
reward at a specific time (Fig. 1a; see Methods). In standard RL algo-                                                                                             
                                                                                                                                                      E (r|t = ∆t) 
rithms, the agent learns to predict future rewards, compressed into                                                   Vi = 1γi ∆t γi 2∆t … γi T   E (r|t = 2∆t)        (3)
a single scalar value, that is, the sum of discounted future rewards                                                                                               
                                                                                                                                                             ⋮     
expected from the current state1:
                                                                                                                                                      E (r|T ) 
                                                    ∞                                                    Where we assumed that timestep transitions are discrete and of size
                                         V (s) = E  ∑ γ trt                                     (1)
                                                    t =0                                               Δt (see Methods). Thus, single-timescale learning projects all the
                                                                                                         timestep-specific expected rewards (E (r|t)) onto a single scalar (Vi)
where V(s) is the value of the state s, rt is reward at time t, γ is the discount                        through exponential discounting (Fig. 1a) and therefore entangles
factor (0 < γ < 1) and E denotes the expectation over stochasticity in the                               reward timing and reward size. When learning with multiple timescales,
environment and actions. This exponentially functional form for the                                      instead of collapsing all future rewards onto a single scalar, there is a
temporal discount (γt) is not arbitrary. It is naturally produced by the                                 vector of value predictions, each computing value with its own discount
TD learning rule, a bootstrapping mechanism that updates the value                                       factor γi (ref. 21):
estimate for state s after transitioning from s to s′ and receiving reward r:
                                                                                                                                       1 γ ∆t γ 2∆t                … γ1T   E (r|t = 0) 
                                                                                                                               V1  
                                                                                                                                            1    1
                                                                                                                                                                                           
                     V (s) ← V (s) + α[r + γV (s ′) − V (s)]                                      (2)                                                                           E (r|t = ∆t) 
                                                                                                                                            ∆t
                                                                                                                                ⋮  = 1 γ2 γ2
                                                                                                                                                  2∆t
                                                                                                                                                                    ⋯ γ2T  
                                                                                                                                 1 ⋮                                      E (r|t = 2∆t) = LE (r|t)              (4)
                                                                                                                               Vn            ⋮                 ⋱ ⋮             ⋮      
where α is the learning rate. This update process converges to the                                                                          ∆t   2∆t                                      
values defined above under very general conditions1 and has been                                                                       1 γ    γ                   ⋯ γnT   E (r|T ) 
                                                                                                                                            n    n                                          
experimentally proven to be an extremely robust and efficient learn-
ing rule in training deep RL systems13 and at characterizing the firing                                    The last equality shows that the array of values learned with multi-
of dopaminergic neurons in the midbrain6–8.                                                              ple discounts (value space in Fig. 1b) corresponds to the z-transform


2 | Nature | www.nature.com
(that is, the discrete Laplace transform (L)) of the array that indicates
the expected reward at all future timesteps (temporal space in Fig. 1b).      Task 3: inferring temporal information before convergence
As the z-transform is invertible, the agent using TD learning with multi-     In single-timescale systems, a high value of the cue could be due to a
ple timescales implicitly encodes the expected temporal evolution of          short delay (tR) or simply because the value estimate in equation (1)
rewards, which can be recovered by applying a fixed, regularized linear       has undergone more positive updates from an initial value of 0 (for
decoder L−1 to the learned values21,37 (Fig. 1b, right panel illustrates a    example, if it has encountered the reward a larger number of times;
situation with one reward per trajectory, but this approach also works        see Extended Data Fig. 1s–w). In multi-timescale systems, the shape
for multiple reward; see Methods and ref. 21).                                of value function across discount factors encodes the proximity to
   RL agents performing multi-timescale learning have been shown to           rewards (Fig. 1b, medium left panel), and this shape is invariant to the
produce performance superior to that of single-timescale agents               number of rewards encountered, to the extent that all value estimates
across a wide range of complex problems19,38. To illustrate the compu-        depart from similar baselines and share similar learning parameters.
tational advantages of multi-timescale representations, we considered         As a result, multi-timescale agents can decode the time of reward (tR)
several example tasks, including a simple linear maze (Fig. 1c–f and          even in situations in which learning is incomplete (task 3; Fig. 1e,f and
Extended Data Fig. 1a–o), branching mazes (Extended Data Figs. 1p–r,          Extended Data Fig. 1; see Methods).
2 and 3a–e), a navigation setting (Extended Data Fig. 2f–i) and a deep
Q-network (DQN) setting (Extended Data Fig. 2j–l). In the linear maze,        Task 4: state-dependent discount factor
the agent navigates through a linear track (a sequence of 15 states),         Single-timescale systems are either myopic or far-sighted, whereas
where it encounters a reward of a certain magnitude (R) at a specific         multi-timescale systems can adjust between myopic and far-sighted
time point (tR) (see Fig. 1c). The value of R and tR changes across epi-      perspectives, leading to more accurate value estimates in incomplete
sodes and remains constant within episodes. Each episode is initiated         learning scenarios. Consider a slight modification of the task in Fig. 1c,
by a cue presented at the initial state (s). Within each episode, the agent   in which, in addition to the large deterministic reward (R = 1), small sto-
first learns the expected future rewards predicted by the cue (that is,       chastic rewards sampled from a Gaussian distribution are perceived at
the valueVγ(s)) using a simple RL algorithm (tabular TD learning) using       every state (see Methods). If the agent experiences the trajectory many
one or multiple discount factors (step 1 in Fig. 1d). Using the learned       times, the noisy rewards average out, so they do not affect the learned
value (or values) associated with the cue, the agent then performs            value of the cue. In task 4, however, the agent experiences the trajectory
various tasks, which involve producing a task-specific report by trans-       only once, so the noisy rewards do affect the values learned with TD
forming the learned values using a decoder network (step 2 in Fig. 1d).       learning. Given the noisy values, the goal in this task was to report the
As some tasks involve complex, non-linear operations over the                 true value of the cue that would arise after experiencing the trajectory
multi-timescale values, we trained a general, non-linear decoder for          an infinite number of times (this is, ignoring the noisy rewards). When tR
each task using policy gradient (see Methods). Our goal is to evaluate        is small, far-sighted estimates not only integrate R but also all the noisy
the advantages of the multi-timescale value representation over the           rewards farther in the future, in contrast to myopic estimates, which
single-timescale value representation, and the degree to which these          assign greater weight to R. However, when tR is large, only far-sighted
advantages can be exploited by a simple, code-agnostic decoder.               estimates can discern R from the noisy rewards. Thus, optimal accuracy
Therefore, in our model, multi-timescale values are not used directly         is only achievable by multi-timescale agents that can estimate tR and
to produce behaviour. Instead, they act as an enriched state represen-        then adjust accordingly between myopic and far-sighted perspectives.
tation from which task-specific behaviour can be subsequently                 Although in this task the uncertainty on the value of the cue arises due to
decoded.                                                                      receiving small noisy rewards at every state, a similar bias also improves
                                                                              the accuracy of value estimates in more realistic learning scenarios,
Task 1: disentangling reward timing and reward magnitude                      in which uncertainty arises due to incomplete exploration of the full
In single-timescale systems, a high value at the cue could signify a small    state space, as we have also shown in more realistic branching mazes,
reward in the near future or a large reward in the distant future. By con-    navigation scenarios (Extended Data Fig. 2f–i) and in the Lunar Lander
trast, the pattern of values across discount factors (middle right panel      environment using a DQN setting in which additional timescales act
in Fig. 1b) is invariant to reward magnitude. As a result, multi-timescale    as auxiliary tasks (Extended Data Fig. 2j–l; see ‘The myopic learning
agents can disentangle reward timing from reward magnitude (task 1            bias’ in Methods).
in Fig. 1e,f) in which the agent reports reward timing independently of          To summarize, in multi-timescale value systems, the vectorized learn-
reward magnitude (Fig. 1f and Extended Data Fig. 1a–c; see Methods).          ing signal robustly contains temporal information independently of
                                                                              reward magnitude and learning conditions. This property empowers
Task 2: learning values with non-exponential temporal                         agents to flexibly adapt their behaviour to novel temporal contingen-
discounts                                                                     cies and focus on either myopic or far-sighted estimates depending
The bootstrapping process of traditional TD value learning naturally          on the current situation.
converges to exponentially discounted values. Although several tasks
can be optimally solved by knowing the exponentially discounted state
values (that is, where the value of a reward at time t decreases as γt),      Discounting across dopaminergic neurons
the optimal discount in a specific task depends on its temporal con-          In the previous section, we demonstrated the computational advan-
tingencies, such as its hazard rate, its horizon, the cost of time and        tages of learning with multiple discount factors for an RL agent. Build-
the uncertainty over time19,38. Indeed, human and animal judgements           ing on these findings, we next investigated whether the brain uses such
are generally more consistent with a hyperbolic discount (that is,            multi-timescale RL. Towards this goal, we examined the activity of
decreasing as 1/(1 + γt)) than an exponential discount9,10. Crucially,        dopaminergic neurons, which are believed to encode the TD error
multi-timescale systems with a diversity of exponential discounts             term in RL algorithms.
implicitly encode the expected reward magnitudes at all future times             To characterize the discounting properties of individual dopamin-
(Fig. 1b), so they can weigh the time-specific expected rewards with          ergic neurons, mice (n = 8; see Extended Data Fig. 10e) were trained in
any chosen discount weights to retrieve the specific discount neces-          a cued delay task27,39, in which on a given trial, one out of four distinct
sitated by the task. Our result shows that only multi-timescale systems       odour cues indicated its associated timing of a water reward (Fig. 2a).
can reliably report the hyperbolic value of the cue given a diversity of      These odour cues were preceded by a trial start cue (green computer
exponential values (task 2; Fig. 1e,f).                                       screen) by 1.25 s. The trial start cue reduced the timing uncertainty of


                                                                                                                        Nature | www.nature.com | 3
Article
a                                                                                         b                                                                                                                          d
                                                                                                       10                                                                                                                             20
                                                                                                                                                                                                                                                            = 0.72




                                          e




                                                                                          Licks per
                                      cu
                                do t




                                                                                           second
                                        r
                                     ta




                                                            d
  Odour




                                                          ar
                                   ur




                                                                                                                                                                                                                     Spikes per
                                  ls
                                                                                                           5




                                                          w
                              ia




                                                                                                                                                                                                                      second
                                                         Re
                            Tr

                               O
                                                                                                                                                                                                                                      10
          A                                                                                                0
                                                                                                               –1.25       0 0.6 1.5                       3.75                                     9.375
          B                                                                                                                                                                                                                            0
                                                                                          c                                                                                    Cue response                                                0           5              10
                                                                                                                                                                         20
          C                                                                                            20                                                                                                                                                   = 0.34




                                                                                          Spikes per




                                                                                                                                                                                                                     Spikes per
                                                                                                                                                                                                                                      20




                                                                                           second




                                                                                                                                                                                                                      second
          D
                                                                                                       10                                                                 0
                                                                                                                                                                               0              0.5
                           5

                                 0


                                                6
                                                5
                                               75



                                                                 5

                                                                                                                                                                                                                                       0
                       .2




                                             0.
                                             1.




                                                                37


                                                                                                           0
                                            3.
                     –1




                                                               9.




                                                                                                               –1.25       0 0.6 1.5                       3.75                                     9.375                                  0      5         10
                                   Time from odour cue (s)
                                                                                                                                                  Time from odour cue (s)                                                                   Reward delay (s)


e                                                   1                       f                                                     h                                                                         j
                           Cue response                                                                                                               1
                                                    0
                            (normalized)                                                                                                                                                                                   Multi-timescale RL




                                                                                                                                  Cue response
                                                                                                                  P < 0.05




                                                                                                                                   (normalized)
                                                    –1                          10                                                                 0.5
                       A       B        C       D
                1                                                                                                                                                                                                         =                   L               E(r ⎜t)
                                                                        Count




                                                                                                                                                                                                                                                   •

                                                                                                                                                      0

                                                                                                                                                                                                                           =                               •

                                                                                0                                                                          0             0.5              1                                                                              t
            20                                                                        –0.05            0        0.05
                                                                                                                                                               Discount factor (s–1)                           Cue                    Discounting Expected
Neuron




                                                                                                  ΔR2                                                                                                       response                   functions    reward
                                                                                                                                                                                                                                        (matrix)    at each
                                                                            g                                                     i
                                                                                                                                                                                                                                                  time point
                                                                                10                                                                     1
                                                                                                                                                                                              Relative            Inverse Laplace transform
                                                                        Count




            40                                                                                                                                                                                 value
                                                                                                                                            Neurons




                                                                                                                                                      20                                                        (regularized pseudo-inverse)
                                                                                                                                                                                                  1

                                                                                 0                                                                    40
                                                     0         1                                                                                                                                                L–1               •               ≈           E(r ⎜t)
                     6

                             5

                                       75

                                             5




                                                                                      0          0.5               1                                                                                0
                    0.

                            1.



                                            37




                                                      Discount
                                   3.

                                           9.




                                                     factor (s–1)                    Discount factor (s )          –1                                      0           4      8        12
                       Reward delay (s)
                                                                                                                                                                       Time (s)                                                   •                ≈

k                                                                                                                                                                                                                                                                        t
                                             Odour A                            Odour B                                     Odour C                                    Odour D
                                              (0.6 s)                            (1.5 s)                                    (3.75 s)                                   (9.375 s)                            Pseudo                   Cue
                                                                                                                                                                                                                                                                  ?
                                                                                                                                                                                                            -inverse              response
                               0.1                                                                             0.05                                       0.05

                                                                0.05
        Test




                    E(r ⎜t) 0.05
                                                                                                                                                                                               l
                                                                                                                                                                                                                                      
                                 0                                 0                                              0                                          0
                               0.1                               0.1                                           0.04                                        0.04
                                                                                                                                                                                               V
(single discount)
     Control




                    E(r ⎜t) 0.05                                0.05                                           0.02                                        0.02
                                                                                                                                                                                                


                                   0                                0                                             0                                            0
                                       0    4     8       12            0       4     8           12                   0     4     8              12               0    4     8      12
                                            Time (s)                            Time (s)                                     Time (s)                                   Time (s)

Fig. 2 | Dopaminergic neurons exhibit a diversity of discount factors that                                                                  and P > 0.05 (light blue)). g, Distribution of inferred discount factors across
enables decoding of reward delays. a, Outline of the cued delay task structure.                                                             neurons (mean discount factor across bootstraps). The colour indicates the
Image of a water droplet was created by googlefonts via SVG Repo under an                                                                   animal. h, Shape of the normalized population response as a function of reward
Apache Licence. b, Anticipatory licking before reward delivery (mean across                                                                 delay. The thick lines denote smoothed fit, the dotted lines indicate theory and
behaviour for all recorded neurons; the shaded error bar indicates 95%                                                                      the dots denote individual neurons. i, Discount matrix. Neurons are sorted as in
confidence interval using bootstrap). n = 8 mice. c, Average PSTH for the                                                                   panel d. j, Outline of the decoding procedure. k, The subjective expected timing
four trial types. The inset shows the firing rate in the 0.5 s following the cue                                                            of future reward E (r|t ) can be decoded from the population responses to the cue
predicting reward delay. The firing rate in the shaded grey box (0.1 s <t < 0.4 s)                                                          predicting reward delay. Decoding based on mean cue responses for test data
was used as the cue response in subsequent analysis. n = 50 dopaminergic                                                                    (top row; see Methods) is better than using a model with a single discount factor
neurons. d, Example cue response fits for two single neurons. e, Normalized cue                                                             (the mean discount factor across the population; bottom row; thin lines (light
responses across the population. For each neuron, the response was normalized                                                               shade) indicate predictions for individual bootstraps, thick lines (dark shade
to the highest response across the four possible delays. The inset shows the                                                                within light shading) indicate mean prediction across bootstraps, and single
inferred discount factor for each neuron. f, Data are better fit by the exponential                                                         dark vertical lines indicate reward timing; see Methods; Extended Data Fig. 4e).
than the hyperbolic model (distance of mean R 2 to the unit line; the shading                                                               l, Model in which the RPE of each dopaminergic neuron contributes to a distinct
indicates significance for single neurons across bootstraps: P < 0.05 (dark blue)                                                           value function (see Methods; Extended Data Fig. 7f–k).



4 | Nature | www.nature.com
the odour cue and ensured that the responses of dopaminergic neurons                      transform (Fig. 2j and Extended Data Fig. 4a–d; see Methods and
to the odour cues were mostly driven by a valuation signal rather than                    ref. 21) and applied it to the cue responses of a dopaminergic neuron
a saliency signal40. Mice showed anticipatory licking before reward                       (computed on the held-out half of the trials). The decoder was able to
delivery. The onset of the anticipatory licking was delayed for trials with               predict reward timing, closely matching the true reward delay (Fig. 2k,
cues predicting longer reward delays, indicating that the mice learned                    top row). This prediction was lost if we shuffled the neuron identities,
the delay contingencies (Fig. 2b). We recorded optogenetically identi-                    indicating that it is not a generic property of the discount matrix
fied single dopaminergic neurons in the ventral tegmental area (n = 78;                   (Extended Data Fig. 4f). We quantified this decoding by computing a
see Methods). We focused our analysis on neurons (n = 50) that passed                     distance metric (using 1-Wasserstein distance) between the true and
the selection criteria (including mean cue response firing rate above 2                   predicted reward delay across conditions (compared with shuffle con-
spikes per second, positive goodness of fit on test data; see Methods).                   trol: P = 1.2 × 10−4 for 0.6-s reward delay and P < 1.0 × 10−20 for the other
As expected from RL theory and the prediction error framework, the                        delays, one-tailed Wilcoxon signed-rank test; Extended Data Fig. 4g;
average responses to the reward cue decreased as the predicted reward                     see Methods). Predictions from the model were more accurate
timing increased27,39 (Fig. 2c and Extended Data Fig. 3a,b). However, cue                 than an alternative model with a single discount factor in which the
responses of individual neurons showed a great diversity of discounting                   response of each neuron is interpreted as a sample from the reward
across the reward delays, ranging from neurons responding strongly                        timing distribution (Pt = 0.6 s = 1, Pt = 1.5 s < 1.0 × 10−31, Pt = 3.75 s = 0.0135 and
only to the cue indicating the shortest delay to neurons with a gradual                   Pt = 9.375 s < 1.0 × 10−14, one-tailed Wilcoxon signed-rank test; Fig. 2k, bot-
decay of their response with cued reward delay (Fig. 2d,e).                               tom row, and Extended Data Fig. 4e; see Methods). Consistent with
   To characterize the discount properties of individual neurons, we                      the above observation that cue responses were fit better with expo-
fit them individually using both an exponential discount model and a                      nential over hyperbolic discounting models, the accuracy of reward
hyperbolic discount model. The exponential model provided a better                        timing decoding was typically higher when using the discount matrix
fit to the responses of neurons than the hyperbolic model (P = 2.2 × 10−5,                from the exponential model than the discount matrix from the hyper-
two-tailed Student’s t-test, comparing the distribution across neurons                    bolic model (Pt = 0.6 s = 1, Pt = 1.5 s < 1.0 × 10−31, Pt = 3.75 s < 1.0 × 10−33 and
of the mean (across bootstraps) difference in R2 between the two fits;                    Pt = 9.375 s < 1.0 × 10−3, one-tailed Wilcoxon signed-rank test; Extended
Fig. 2f and Extended Data Fig. 3c–e; see Methods) contrary to a previ-                    Data Fig. 6a–e). Furthermore, the decoding performance was compa-
ous observation39. Organism-level hyperbolic-like discounting can,                        rable with simulated data with matched trial numbers, indicating that
therefore, arise from the diversity of exponential discounting in single                  the remaining uncertainty in decoded reward timing is primarily driven
neurons, as discussed above with artificial agents (Fig. 2d; see also                     by limited sample size in the data (for example, the number of neurons
refs. 12,19,33). This view is consistent with the wide distribution of                    and the number of trials per condition; Extended Data Fig. 6f,g; see
inferred discount factors obtained across the population (0.56 ± 0.21 s−1,                Methods). We performed the decoding analysis at the single-animal
mean ± s.d.; Fig. 2g). Fits to simulated data suggest that our estimate of                level for two of the animals for which we had a sufficient number of
inferred parameters is robust and primarily constrained by the num-                       neurons and observed decoding of subjective reward timing (Extended
ber of trials (Extended Data Fig. 3f–j; see Methods). Furthermore, we                     Data Figs. 5g and 10e; Methods).
measured behavioural discounting using the anticipatory lick rate                            Together, these results establish that dopaminergic neurons com-
and show that it is not correlated to the discounting measured from                       pute prediction errors with a heterogeneity of discount factors and
single dopaminergic neurons (Extended Data Fig. 5a–f; see Methods).                       show that the structure in this heterogeneity can be exploited by down-
   As we have shown above, artificial agents equipped with diverse                        stream circuits to decode reward timing.
discount factors exhibit various advantages. One key aspect contribut-
ing to these advantages is their unique ability to independently extract
reward timing information, which is lacking in single-timescale agents.                   Ramping heterogeneity and discounting
We next asked whether dopaminergic neurons provide a popula-                              In the task above (Fig. 2), prediction errors in dopaminergic neurons
tion code in which the structured heterogeneity across the population                     were measured through discrete transitions in the value functions at
enables decoding of reward timing or the expected reward across                           the time of cue. In more naturalistic environments, value might change
time, E (r|t) . Mathematically, this transformation can be achieved                       more smoothly, for example, when an animal approaches a goal41. In
by the inverse Laplace transform (or its discrete equivalent the z-                       these tasks, ramps in dopaminergic signalling have been initially inter-
transform)21,34,37 (Fig. 2j). In our dataset, the dopaminergic cue responses              preted as quantifying value functions41,42, but have recently been shown
for each reward delay exhibited unique shapes as a function of discount                   to conform to the predictions of the TD learning model. Specifically,
factors, suggesting that reward timing information is embedded in the                     these ramps can be understood as moment-by-moment changes in
dopaminergic population responses (Fig. 2h; compare with Fig. 1b).                        values or as TD error along an increasingly convex value function in
The temporal horizon across the population, which underlies these                         which the derivative is also increasing43–45. Here we show that some of
cue responses, can be visualized through the discount matrix, which                       the diversity in ramping activity across neurons can be understood as
indicates for each neuron the relative value of a future reward depend-                   evidence for multi-timescale RL across dopaminergic neurons.
ing on the inferred discount factor (Fig. 2i).                                               We analysed the activity of optogenetically identified dopaminergic
   If the dopaminergic population code is consistent with the Laplace                     neurons (n = 90 from n = 13 mice; Extended Data Fig. 10e; see Methods
code explored above (Fig. 1) and each dopaminergic neuron contributes                     and ref. 44) while mice traversed along a linear track in virtual real-
to a distinct value estimate (Fig. 2l and Extended Data Fig. 7f–k), reward                ity. Although mice were free to locomote, their movements did not
timing should be recoverable from the cue responses of dopaminergic                       affect the dynamics of the scene (see Methods and ref. 44 for details).
neurons with a regularized discrete inverse Laplace transform of the                      At trial onset, a linear track appeared, the scene moved at continuous
neural activity (which does not require training a decoder). In our task,                 speed, and reward was delivered 7.35 s after motion onset (Fig. 3a). The
we can use the TD-error-driven cue responses (instead of the value in                     slope of ramping across neurons was on average positive (Fig. 3b), but
equation (4)) as they are driven by the discounted future value, that                     single neurons exhibited a diversity of ramping activity (Fig. 3b, inset,
is, δt cue = γ ∆tVt cue+∆ t + C , as rt cue = Vt cue−∆t = 0; see Methods). This implies   and 3e,f) ranging from monotonic upwards and downwards ramps to
that the right-hand side of equation (4) can be approximated by the                       non-monotonic ramps.
population dopamine responses. We used a pseudo-inverse of the                               We hypothesized that this seemingly puzzling heterogeneity can be
discount matrix (computed using half of all trials) based on regularized                  understood as a signature of multi-timescale RL. Considering that the
singular value decomposition to approximate the inverse Laplace                           value function is set by the limits on the precision of internal timing


                                                                                                                                          Nature | www.nature.com | 5
Article
a                                                                                                                                                                                                                           b
                                 Virtual reality                                          Computer monitor                                                                                                                                                                                      P < 0.05




                                                                                                                                                                                                                                                                                Count
                                                                                                                                                                                                                                                                                        10

                                                                                                                               Start                                                               Goal                                                           n = 90                 0
                                                                                                                                                                                                                                                  0.5




                                                                                                                                                                                                                                                                                               0
                                                                                                                                                                                                                                                                                              .5

                                                                                                                                                                                                                                                                                               5
                                                                                                                                                                                                                          Normalized




                                                                                                                                                                                                                                                                                                 0.
                                                                                                                                                                                                                          spike rate




                                                                                                                                                                                                                                                                                          –0
                                                                                                                                                                                                                                                                                               Slope


                                                                                                                                                                                                                                                      0
                                                                                          Wheel
                                                                                                                                                                                                                                                            –7.35                 –4                         0
                                                                                                                                                                                                                                                                            Time to reward (s)

c                                                      d                                                                                                      e                                                     f                                               Fit to the data
                                                                                                                                                                                               Data    Fit ()                                                              
                        4                                                                                                                                                         4      = 0.86
                                                                                     80                                                                                                                                                          80




                                                                                                                                                                                                                  (sorted by discount factors)
                                                      (sorted by discount factors)




                        2                                                                                                                                                         2
Spikes per second




                                                                                                                                                            Spikes per second
                    10                                                                                                                                                          10       = 0.45




                                                                                                                                                                                                                           Neurons
                                                               Neurons




                        4                                                            40                                                                                           4                                                              40
                                                                                                                              Normalized
                                                                                                                                                                                                                                                                                                       TD error
                        4                                                                                                     spike rate                                          4
                                                                                                                                  Max                                                                                                                                                                     Max
                                                                                                                                  0                                                      = 0.34                                                                                                           0
                        2                                                                                                                                                         2
                                                                                      1                                           –Max                                                                                                            1                                                        –Max
                             –4        –2        0                                         –4         –2          0                                                                    –4        –2        0                                              –4         –2          0
                              Time to reward (s)                                                                                                                                        Time to reward (s)                                                  Time from reward (s)
                                                                                             Time from reward (s)

                                                     Dopamine neurons                                                                                                                                                                                 k                 Best fit value function
                             Value function                                                                               TD error
                                                         with diverse                                                      ⋅                                                                                                                                                        V(t)
                                  V(t)                                                                             = rt + V(t) + ln i ⋅ V(t)                                                                                                                     1
                                                     discount factors (i)
g                                                                                                                                                 h




                                                                                                                                                                                                                                                           Value
                                                                                                                                                                                              
                                                                                                                                                                                 1
                                                                                                                                                      Discount factor




                    1                                                                i = 0.92                   1
                                                                                                                                                                                                           TD error                                                0
                                                                                                      TD error




                                                                                                                                                                                0.6                                                                                     –4       –2       0
Value




                                                                                     i = 0.46                   0                                                                                           Max
                                                                                                                                                                                                             0                                                          Time from reward (s)
                                       = 0.46                                       i = 0.27                   –1
                    0                                                                                                 –4               0                                        0.2                          –Max                                     l                                  i
                                                                                                                                                                                                                                                                   20
                        –4                    0                                                                                                                                       –4               0
                                                                                                                        Time to reward


                                                                                                                                                                                                                                                          Count
                             Time to reward                                                                                                                                             Time to reward
                                                                                                                                                                                                                                                                   10

i                                                                                                                                                  j                                                                                                                0
                                                                                                                                                                                              
                                                                                                                                                                                 1                                                                                      0               0.5            1
                    1
                                                                                                                                                 Discount factor




                                                                                     i = 0.92                   1                                                                                                                                                      Discount factors (s–1)
                                                                                                      TD error




                                                                                                                                                                                                           TD error
Value




                                                                                     i = 0.46                   0                                                      0.75                                 Max                           m                                    
                                                                                     i = 0.27                   –1                                                                                          0
                    0                                                                                                 –4               0                                                                                                   V
                                                                                                                                                                                0.5                          –Max
                        –4                    0
                                                                                                                        Time to reward                                                –4               0
                             Time to reward                                                                                                                                                                                                                                                                  C
                                                                                                                                                                                        Time to reward                                     

Fig. 3 | The diversity of discount factors across dopaminergic neurons                                                                           (dark red), non-monotonic (red) and monotonic downwards (light red) ramps.
explains qualitatively different ramping activity. a, Experimental setup.                                                                        d, Individual neurons across the population exhibit a spectrum of diversity in
View of the virtual reality corridor at movement initiation (left). Schematics                                                                   their ramping activity. Neurons are sorted according to the inferred discount
of the experimental setup (middle and right). The mouse image in the diagram                                                                     factor from the common value function model (panel k). e, Example model fits
of the experimental setup in the right panel was created by Gil Costa under a                                                                    for the single neurons shown in panel c. f, The model captures the diversity of
Creative Commons licence CC BY 4.0. b, Average activity of single dopaminergic                                                                   ramping activity across the population. Neurons are ordered by the inferred
neurons (n = 90) exhibits an upwards ramp in the last few seconds of the track                                                                   discount factor as in panel d. g,h, Diversity of ramping as a function of discount
before reward delivery. The error bars represent s.e.m. across neurons. The                                                                      factor for an exponential value function. i,j, Diversity of ramping as a function
inset shows that the slope of the activity ramp (computed between the two black                                                                  of discount factor for a cubic value function. k, Inferred value function. The thin
horizontal ticks in main panel) is positive on average but varies across neurons                                                                 grey lines denote the inferred value function for each bootstrap. The thick blue
(for population, mean slope = 0.097; P = 0.0175. For single neurons, positive and                                                                line indicates the mean over bootstraps. l, Histogram of inferred discount
P < 0.05: n = 53; negative and P < 0.05: n = 32; P > 0.05: n = 5, two-tailed Student’s                                                           factors. Mean ± s.d. of 0.42 ± 0.23. m, Model in which the value function used
t-test). Image of a water droplet in panels a,b was created by googlefonts via SVG                                                               for the RPE computation is shared across dopaminergic neurons (see Methods
Repo under an Apache Licence. c, Example single neurons showing diverse                                                                          and Extended Data Fig. 7f–k).
ramping activity in the final approach to reward, including monotonic upwards



mechanisms and the reduction in uncertainty due to visual feed-                                                                                  the environment and therefore share a common value function (Fig. 3m;
back45,46, we first assumed that heterogeneous dopaminergic neurons                                                                              see Methods). Depending on the shape of this value function, governed
contribute to learning a common model of the value of the states in                                                                              by the statistics of the environment being learned, the TD error from


6 | Nature | www.nature.com
neurons with different discount factors will exhibit different types of               a                                                      b
                                                                                                           1                                        4
activity ramps. At a given time, the sign of the TD error will depend on
the relative scale of the upcoming increase in value and the reduction




                                                                                      (cued delay task)
of this future value due to discounting. Given an increase in value 1/γo




                                                                                       Discount factor
(with γo < 1), a neuron with a discount factor smaller, equal or larger




                                                                                                                                             P(r)
than γo will experience a negative, zero or positive TD error, respectively                               0.5                                       2
(Extended Data Fig. 7a; see Methods). For an exponential value function
(Fig. 3g, left panel), in which the value increases by a fixed factor 1 at
                                                                              γo
every timestep, a neuron with discount factor γo will have no TD error                                                           r = 0.45
during the entire visual scene (red line, Fig. 3g,h). A neuron with a higher                                                    P = 0.0013
                                                                                                           0                                        0
(or lower) discount factor than γo will experience an upwards (or down-                                         0         0.5            1              –0.5         0            0.5
wards) monotonic ramp in its activity (darker and lighter red line in                                               Discount factor                        Correlation coefficient
                                                                                                                    (virtual reality)
Fig. 3g,h, respectively). However, if the value function is non-exponential
(for example, cubic as a function of distance to reward (Fig. 3i, left                Fig. 4 | Discount factors of single dopaminergic neurons are correlated
panel) or hyperbolic as a function of distance to reward (Extended Data               across behavioural contexts. a, Correlation across neurons between the
Fig. 7b, left panel)), there will not be a neuron whose discount factor is            discount factors inferred in the virtual reality task and the discount factors
able to match the increases in value function at all timesteps. Neurons               inferred in the cued delay task (Spearman’s rank correlation, one-tailed
with high or low discount factors will still ramp upwards or downwards                Student’s t-test). b, Distribution of Spearman’s rank correlations between the
(darker and lighter red line in Fig. 3i,j and Extended Data Fig. 7b, respec-          discount factors across the two tasks for randomly sampled pairs of bootstrap
                                                                                      estimates (0.34 ± 0.104, mean ± s.d.; P < 1.0 × 10 −30, one-tailed Student’s t-test).
tively), but neurons with intermediate discount factors will exhibit
non-monotonic ramping (red line, Fig. 3i,j and Extended Data Fig. 7b)
as observed in the neural data.
   To fit this model to the dopaminergic neurons, we used a boot-                     adaptivity. Specifically, the tuning of single dopaminergic neurons,
strapped constrained optimization procedure on a continuous for-                      controlled by the sensitivity to reward size or the discount factor, could
mulation of the TD error45,47 (δi(t) = bi + αi(γi dt dV (t)/dt − γi dtln(γi)V (t));   be either a circuit property and therefore task and context specific or
see Methods) by fitting a non-parametric common value function and                    a cell-specific property, with the contribution of different neurons
neuron-specific gains, baselines and discount factors. Although the                   recruited according to task demands. However, measurements of
gain and baseline activity scale the range of activity, only the interac-             tuning diversity at the single-neuron level are usually done in a single
tion between the value function and the discount factor affects the                   behavioural task20,51,52, leaving open the question of this implementa-
shape of the TD error across time (see Methods). The heterogeneity                    tion across contexts.
of ramping activity across single neurons is explained (Fig. 3e,f) by a                  Here we characterized discount factors across two behavioural
common convex value function (Fig. 3k) and a diversity of discount                    tasks, and a subset (n = 43) of the single neurons analysed above (Figs. 2
factors across single neurons (Fig. 3l). We did not observe a significant             and 3) was recorded on the same day in both behavioural tasks. Using
correlation neither between inferred parameters and the mediolateral                  this dataset, we found that the discount factors inferred independently
position of the implanted electrodes (Extended Data Fig. 7c–e; although               across the two behavioural tasks are correlated (Fig. 4a,b). Furthermore,
we did not sample extensively lateral positions) nor with licking behav-              in the cued delay task, we were able to decode subjective reward tim-
iour before reward delivery (a measure of behavioural discounting;                    ing from population cue responses using the discount matrix built
Extended Data Fig. 8a–d; see Methods). Furthermore, the model fit                     from the discount factors inferred in the virtual reality task (Pt = 0.6 s = 1,
was robust when applied at the single-animal level for the two animals                Pt = 1.5 s < 1.1 × 10−20, Pt = 3.75 s < 3.8 × 10−20 and Pt = 9.375 s < 2.9 × 10−5, compared
with sufficient numbers of neurons (Extended Data Fig. 8e–j; see Meth-                with shuffled data; Extended Data Fig. 10a–d; see Methods). These results
ods). So far, we proposed a descriptive model with a common value                     suggest that the discount factor (or its ranking) is a cell-specific prop-
function across neurons, suggesting that the prediction errors of sin-                erty and strongly constrains the biological implementation of multi-
gle neurons are pooled to estimate a single value function. Recent                    timescale RL in the brain.
models for distributed prediction errors across dopaminergic neurons
have instead used parallel loops in which individual neurons con-
tribute to estimating separate value functions20,21,23,48–50. We obtained             Discussion
similar results in such a model in which neurons estimate separate                    In this work, we have analysed the unique computational benefits of
value functions and instead share a common expectation of reward                      multi-timescale RL agents and shown that we can explain multiple
timing (see Methods; Extended Data Fig. 9). We can reconcile these                    aspects of the activity of dopaminergic neurons through that lens.
two models as being two edge cases of a model in which, across                           The understanding of dopaminergic neurons as computing a
independent value estimators, there is a relative amount of mixing                    reward prediction error from TD RL algorithms has transformed our
between independent estimates and a common value signal (see the                      understanding of their function. However, recent experimental work
section ‘Mixing in distributed RL models’ in Methods; Extended Data                   expanding the anatomical locations of recordings and the task designs
Fig. 7f–k).                                                                           has shown heterogeneity in dopamine responses that is not readily
   Together, these results show that diversity in slow changes in activity            explained within the canonical TD framework42,53,54. However, a number
across a single neuron (known as dopamine ramps) in environments                      of these seemingly anomalous findings can be reconciled and inte-
with gradual changes in value can be explained by a diversity of discount             grated within extensions of the RL framework, further reinforcing the
factors and is a signature of multi-timescale RL.                                     power and versatility of the TD theory in capturing the intricacies of
                                                                                      brain learning mechanisms23,24,48,55,56. In this work, we have revealed an
                                                                                      additional source of heterogeneity across dopaminergic neurons: they
Correlated discount factors across tasks                                              encode prediction errors across multiple timescales. Together, these
Distributional RL and other distributed RL formulations provide agents                results indicate that at least some of the heterogeneity observed in
with greater flexibility as they allow agents to adapt risk sensitivity               dopamine responses reflects variations in key parameters within the RL
and discounting to the statistics of the environment17,19,21,23. However,             framework. Thus, these results indicate that the dopamine system uses
they leave open the question of the biological implementation of this                 ‘parameterized vector prediction errors’, including a discrete Laplace


                                                                                                                                               Nature | www.nature.com | 7
Article
transform of the future temporal evolution of the reward function,         1.    Sutton, R. S. & Barto, A. G. Reinforcement Learning 2nd edn (MIT Press, 2018).
                                                                           2.    Tesauro, G. Temporal difference learning and TD-Gammon. Commun. ACM 38, 58–68
allowing for the learning and representation of richer information than          (1995).
what can be achieved with scalar prediction errors in the traditional      3.    Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518,
RL framework.                                                                    529–533 (2015).
                                                                           4.    Silver, D. et al. Mastering the game of Go with deep neural networks and tree search.
   The constraint on the anatomical implementation of multi-timescale            Nature 529, 484–489 (2016).
RL suggested by the alignment of discount factors between the two          5.    Wurman, P. R. et al. Outracing champion Gran Turismo drivers with deep reinforcement
tasks could also inform algorithm design. Adapting the discount fac-             learning. Nature 602, 223–228 (2022).
                                                                           6.    Schultz, W., Dayan, P. & Montague, P. R. A neural substrate of prediction and reward.
tor has been used to improve performance in several algorithms, with             Science 275, 1593–1599 (1997).
proposed methods ranging from meta-learning an optimal discount            7.    Schultz, W. Neuronal reward and decision signals: from theories to data. Physiol. Rev. 95,
factor57, learning state-dependent discount factors58 or combining               853–951 (2015).
                                                                           8.    Cohen, J. Y., Haesler, S., Vong, L., Lowell, B. B. & Uchida, N. Neuron-type-specific signals
parallel exponentially discounting agents19,33,36. Our results provide           for reward and punishment in the ventral tegmental area. Nature 482, 85–88 (2012).
evidence supporting the third model, but the recruitment mechanisms        9.    Ainslie, G. Specious reward: a behavioral theory of impulsiveness and impulse control.
of the neurons to adapt the global discounting function with task or             Psychol. Bull. 82, 463–496 (1975).
                                                                           10.   Frederick, S., Loewenstein, G. & O’Donoghue, T. Time discounting and time preference: a
context and the link between anatomical location and discounting30               critical review. J. Econ. Lit. 40, 351–401 (2002).
and the contribution of other neuromodulators, such as serotonin59,60,     11.   Laibson, D. Golden eggs and hyperbolic discounting. Q. J. Econ. 112, 443–478 (1997).
to this adaptation remain open questions. Similarly, the contribution      12.   Sozou, P. D. On hyperbolic discounting and uncertain hazard rates. Proc. R. Soc. London.
                                                                                 B 265, 2015–2020 (1998).
of this vectorized error signal on the downstream temporal representa-     13.   Botvinick, M. et al. Reinforcement learning, fast and slow. Trends Cogn. Sci. 23, 408–422
tions26,28 remains to be explored.                                               (2019).
   Understanding how this recruitment occurs will be a key step            14.   Redish, A. D. Addiction as a computational process gone awry. Science 306, 1944–1947
                                                                                 (2004).
towards a mechanistic understanding of the contribution of this            15.   Lempert, K. M., Steinglass, J. E., Pinto, A., Kable, J. W. & Simpson, H. B. Can delay discounting
timescale diversity to calibration and miscalibration in intertemporal           deliver on the promise of RDoC? Psychol. Med. 49, 190–199 (2019).
choices. There has been a conundrum that RL theories use exponen-          16.   Sutton, R. S. et al. Horde: a scalable real-time architecture for learning knowledge from
                                                                                 unsupervised sensorimotor interaction. In The 10th International Conference on Autonomous
tial discounting, whereas humans and animals often exhibit hyper-                Agents and Multiagent Systems Vol. 2 761–768 (International Foundation for Autonomous
bolic discounting. A previous study, which examined discounting in               Agents and Multiagent Systems, 2011); https://dl.acm.org/doi/10.5555/2031678.2031726
                                                                           17.   Bellemare, M. G., Dabney, W. & Rowland, M. Distributional Reinforcement Learning
dopaminergic neurons, has argued that single dopaminergic neu-
                                                                                 (MIT Press, 2023).
rons exhibit hyperbolic discounting39. However, they used uncued           18.   Jaderberg, M. et al. Reinforcement learning with unsupervised auxiliary tasks. In
reward responses for zero reward delay, probably biasing the esti-               International Conference on Learning Representations (ICLR, 2017).
                                                                           19.   Fedus, W., Gelada, C., Bengio, Y., Bellemare, M. G. & Larochelle, H. Hyperbolic discounting
mate towards hyperbolic (as responses to unpredicted rewards are
                                                                                 and learning over multiple horizons. Preprint at https://arxiv.org/abs/1902.06865 (2019).
typically large and potentially contaminated by salience signals). By      20.   Dabney, W. et al. A distributional code for value in dopamine-based reinforcement
contrast, our data are consistent with exponential discounting at the            learning. Nature 577, 671–675 (2020).
                                                                           21.   Tano, P., Dayan, P. & Pouget, A. A local temporal difference code for distributional
level of single neurons, suggesting that RL machinery defined by each
                                                                                 reinforcement learning. Adv. Neural Inf. Process. Syst. 1146, 13662–13673 (2020).
dopaminergic neuron conforms to the rules of a simple RL algorithm.        22.   Brunec, I. K. & Momennejad, I. Predictive representations in hippocampal and prefrontal
Hyperbolic-like discounting can occur when these diverse exponen-                hierarchies. J. Neurosci. 42, 299–312 (2022).
                                                                           23.   Lowet, A. S., Zheng, Q., Matias, S., Drugowitsch, J. & Uchida, N. Distributional
tial discounting are combined at the organism level12,14,33. More gen-
                                                                                 reinforcement learning in the brain. Trends Neurosci. 43, 980–997 (2020).
erally, the relative contribution of multiple timescales to the global     24.   Masset, P. & Gershman, S. J. in The Handbook of Dopamine (Handbook of Behavioral
computation governs the discount function at the organism level                  Neuroscience) Vol. 32 (eds Cragg, S. J. & Walton, M.) Ch. 24 (Academic Press, 2025).
                                                                           25.   Buhusi, C. V. & Meck, W. H. What makes us tick? Functional and neural mechanisms of
and should be calibrated to the uncertainty in the hazard rate of the
                                                                                 interval timing. Nat. Rev. Neurosci. 6, 755–765 (2005).
environment12.                                                             26.   Tsao, A., Yousefzadeh, S. A., Meck, W. H., Moser, M.-B. & Moser, E. I. The neural bases for
   Appropriately recruiting the heterogeneity of discount factors                timing of durations. Nat. Rev. Neurosci. 23, 646–665 (2022).
                                                                           27.   Fiorillo, C. D., Newsome, W. T. & Schultz, W. The temporal precision of reward prediction
is therefore important to adapt to the temporal uncertainty of the               in dopamine neurons. Nat. Neurosci. 11, 966–973 (2008).
environment. This view draws parallels with the distributional RL          28.   Mello, G. B. M., Soares, S. & Paton, J. J. A scalable population code for time in the striatum.
hypothesis that naturally fits with current work on anhedonia, as a              Curr. Biol. 25, 1113–1122 (2015).
                                                                           29.   Soares, S., Atallah, B. V. & Paton, J. J. Midbrain dopamine neurons control judgment of
miscalibration of optimism and pessimism can lead to biases in the               time. Science 354, 1273–1277 (2016).
learned value20. Miscalibration of the discounting spectrum can lead       30.   Enomoto, K., Matsumoto, N., Inokawa, H., Kimura, M. & Yamada, H. Topographic
to excessive patience or impulsivity. A bias in this distribution due to         distinction in long-term value signals between presumed dopamine neurons and
                                                                                 presumed striatal projection neurons in behaving monkeys. Sci. Rep. 10, 8912 (2020).
genetical, developmental or transcriptional factors could bias the         31.   Mohebi, A., Wei, W., Pelattini, L., Kim, K. & Berke, J. D. Dopamine transients follow a striatal
learning at the level of the organism towards short-term or long-term            gradient of reward time horizons. Nat. Neurosci. 27, 737–746 (2024).
goals. Behaviourally such bias would manifest itself as an apparent        32.   Kiebel, S. J., Daunizeau, J. & Friston, K. J. A hierarchy of time-scales and the brain. PLoS
                                                                                 Comput. Biol. 4, e1000209 (2008).
impulsivity or lack of motivation, leading to a potential mechanistic      33.   Kurth-Nelson, Z. & Redish, A. D. Temporal-difference reinforcement learning with
interpretation of these maladaptive behaviours. Similarly, this view             distributed representations. PLoS ONE 4, 7362 (2009).
could guide the design of algorithms that recruit and leverage these       34.   Shankar, K. H. & Howard, M. W. A scale-invariant internal representation of time. Neural
                                                                                 Comput. 24, 134–193 (2012).
adaptive temporal predictions.                                             35.   Tanaka, C. S. et al. Prediction of immediate and future rewards differentially recruits
   Our study has established a new paradigm to understand the func-              cortico-basal ganglia loops. Nat. Neurosci. 7, 887–893 (2004).
tional role of prediction error computation in dopaminergic neurons,       36.   Sherstan, C., Dohare, S., MacGlashan, J., Günther, J. & Pilarski, P. M. Gamma-Nets:
                                                                                 generalizing value estimation over timescale. In Proceedings of the AAAI Conference on
and opens new avenues to develop mechanistic explanations for defi-              Artificial Intelligence 34, 5717–5725 (2020).
cits in intertemporal choice in disease and inspire the design of new      37.   Momennejad, I. & Howard, M. W. Predicting the future with multi-scale successor
algorithms.                                                                      representations. Preprint at bioRxiv https://doi.org/10.1101/449470 (2018).
                                                                           38.   Reinke, C., Uchibe, E. & Doya, K. Average reward optimization with multiple discounting
                                                                                 reinforcement learners. In Neural Information Processing. ICONIP 2017. Lecture Notes in
                                                                                 Computer Science (eds Liu, D. et al.) 789–800 (Springer, 2017).
Online content                                                             39.   Kobayashi, S. & Schultz, W. Influence of reward delays on responses of dopamine
                                                                                 neurons. J. Neurosci. 28, 7837–7846 (2008).
Any methods, additional references, Nature Portfolio reporting summa-      40.   Schultz, W. Dopamine reward prediction-error signalling: a two-component response.
ries, source data, extended data, supplementary information, acknowl-            Nat. Rev. Neurosci. 17, 183–195 (2016).
edgements, peer review information; details of author contributions        41.   Howe, M. W., Tierney, P. L., Sandberg, S. G., Phillips, P. E. M. & Graybiel, A. M. Prolonged
                                                                                 dopamine signalling in striatum signals proximity and value of distant rewards. Nature
and competing interests; and statements of data and code availability            500, 575–579 (2013).
are available at https://doi.org/10.1038/s41586-025-08929-9.               42.   Berke, J. D. What does dopamine mean? Nat. Neurosci. 21, 787–793 (2018).



8 | Nature | www.nature.com
43. Gershman, S. J. Dopamine ramps are a consequence of reward prediction errors. Neural              55. Gershman, S. J. et al. Explaining dopamine through prediction errors and beyond.
    Comput. 26, 467–471 (2014).                                                                           Nat. Neurosci. 27, 1645–1655 (2024).
44. Kim, H. G. R. et al. A unified framework for dopamine signals across timescales. Cell 183,        56. Watabe-Uchida, M. & Uchida, N. Multiple dopamine systems: weal and woe of dopamine.
    1600–1616.e25 (2020).                                                                                 Cold Spring Harb. Symp. Quant. Biol. 83, 83–95 (2018).
45. Mikhael, J. G., Kim, H. R., Uchida, N. & Gershman, S. J. The role of state uncertainty in the     57. Xu, Z., van Hasselt, H. P. & Silver, D. Meta-gradient reinforcement learning. In Advances in
    dynamics of dopamine. Curr. Biol. 32, 1077–1087.e9 (2022).                                            Neural Information Processing Systems Vol. 31 (Curran Associates, 2018).
46. Guru, A. et al. Ramping activity in midbrain dopamine neurons signifies the use of a              58. Yoshida, N., Uchibe, E. & Doya, K. Reinforcement learning with state-dependent
    cognitive map. Preprint at bioRxiv https://doi.org/10.1101/2020.05.21.108886 (2020).                  discount factor. In 2013 IEEE International Conference on Development and Learning
47. Doya, K. Reinforcement learning in continuous time and space. Neural Comput. 12,                      and Epigenetic Robotics (ICDL) https://ieeexplore.ieee.org/document/6652533
    219–245 (2000).                                                                                       (IEEE, 2013).
48. Lee, R. S., Sagiv, Y., Engelhard, B., Witten, I. B. & Daw, N. D. A feature-specific prediction    59. Doya, K. Metalearning and neuromodulation. Neural Netw. 15, 495–506 (2002).
    error model explains dopaminergic heterogeneity. Nat. Neurosci. 27, 1574–1586 (2024).             60. Tanaka, S. C. et al. Serotonin differentially regulates short- and long-term prediction of
49. Cruz, B. F. et al. Action suppression reveals opponent parallel control via striatal circuits.        rewards in the ventral and dorsal striatum. PLoS ONE https://doi.org/10.1371/journal.
    Nature 607, 521–526 (2022).                                                                           pone.0001333 (2007).
50. Millidge, B., Song, Y., Lak, A., Walton, M. E. & Bogacz, R. Reward bases: a simple mechanism
    for adaptive acquisition of multiple reward types. PLoS Comput. Biol. 20, e1012580 (2024).        Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in
51. Engelhard, B. et al. Specialized coding of sensory, motor and cognitive variables in VTA          published maps and institutional affiliations.
    dopamine neurons. Nature 570, 509–513 (2019).
52. Eshel, N., Tian, J., Bukwich, M. & Uchida, N. Dopamine neurons share common response              Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this
    function for reward prediction error. Nat. Neurosci. 19, 479–486 (2016).                          article under a publishing agreement with the author(s) or other rightsholder(s); author
53. Cox, J. & Witten, I. B. Striatal circuits for reward learning and decision-making. Nat. Rev.      self-archiving of the accepted manuscript version of this article is solely governed by the
    Neurosci. 20, 482–494 (2019).                                                                     terms of such publishing agreement and applicable law.
54. Collins, A. L. & Saunders, B. T. Heterogeneity in striatal dopamine circuits: form and function
    in dynamic reward seeking. J. Neurosci. Res. https://doi.org/10.1002/jnr.24587 (2020).            © The Author(s), under exclusive licence to Springer Nature Limited 2025




                                                                                                                                                              Nature | www.nature.com | 9
Article
Methods                                                                         only the expected sum of discounted rewards, as in traditional RL, but
                                                                                also the expected reward at all future timesteps, although the latter lies
Animal care and surgical procedures                                             in a different space, analogous to the frequency and temporal spaces
The mouse behavioural and electrophysiological data presented here              of the Fourier transform.
were collected as part of a previous study in which all experimental
procedures are described in detail44. As described in this study, all           Decoding tasks
procedures were performed in accordance with the US National Insti-             The four tasks in Fig. 1e were designed with a similar structure. In the
tutes of Health Guide for the Care and Use of Laboratory Animals and            four tasks, the agent first performs N backups of tabular TD learn-
approved by the Harvard Animal Care and Use Committee.                          ing (equation (4) in the previous section) on the experimental states
   We used a total of 13 adult C57BL6/J DAT-Cre male mice. Mice were            (Fig. 1c). Then, the learned values for the cue s are input into a policy gra-
backcrossed for over five generations with C57BL6/J mice, Animals were          dient network with one hidden layer of 32 units, and a ReLU non-lineary
singly housed after surgery on a reverse 12-h dark–12-h light cycle (dark       (Fig. 1d, step 2). The policy gradient network receives in its input the val-
from 7:00 to 19:00). Single dopaminergic neurons were optogenetically           ues learned by TD learning and reports in its output the corresponding
identified using custom-built micro-drives with eight tetrodes and              estimate for each task. The policy gradient network was trained across
an optical fibre as described in our previous study44. Significance was         2,000 episodes, after which we evaluated the accuracy of its report.
assessed using the stimulus-associated spike latency test61.                       The precise structure of each episode depends on the task (see details
   All mice (n = 13) were used in the virtual reality task and 8 of those       below). In general, in each episode, the agent learned values from scratch
were also used in the cued delay task. The targeted mediolateral loca-          using TD learning for a specific experimental condition (that is, a Markov
tion varied from 320 µm to 1,048 µm for neurons recorded in the vir-            decision process (MDP)), and the policy gradient network maximized
tuality task and for neurons recorded in the cued delay task. Neurons           its reporting performance across episodes. Thus, for each episode i,
recorded at mediolateral position of more than 900 µm were excluded             the policy (πθ) was a map from the learned multi-timescale values (Vγi)
from the analysis as they were considered to be in the substantia nigra         to actions (ai). The parameters (θ) of the policy gradient network were
pars compacta. For experimental reasons, experimenters were not                 optimized to maximize reporting accuracy across episodes (the specific
blinded to the identity of the mice. Sample size was maximized given            measure to report depends on the experimental condition). The param-
experimental constrains.                                                        eters were learned by optimizing the traditional policy gradient loss,
                                                                                using an Adam optimizer with a learning rate of 0.001 to maximize the
RL at multi-timescales                                                          task-specific expected return J(πθ) of the policy πθ:
In standard RL, the value of a state s under a given policy π is defined
as the expected sum of discounted future rewards:                                                                 N                      
                                                                                               ∇θ J (πθ) = EB πθ ∑ ∇θ logπθ(ai | Vγi)Ci                (9)
                                                                                                                  i =1                  
                                   ∞            
                        V (s) = E  ∑ γ trt|s , π                       (5)
                                   t =0                                       where B is a batch of n = 100 episodes and Ci is a RL binary signal indicat-
                                                                                ing whether the report (ai; the output of the network) was correct or
   The discount factor γ (whose value is between 0 and 1) is a fixed factor     incorrect for episode i, given the learned multi-timescale valuesVγi. To
at each timestep devaluating future rewards. This exponentially func-           tackle the exploration–exploitation problem, we extended the policy
tional form for the temporal discount is not arbitrary. This temporal           using ε-greedy, with ε = 0.3 (performance is reported with ε = 0). We
discount is naturally produced by the TD learning rule, a bootstrapping         used a decoder trained with RL methods instead of supervised learning
mechanism that updates the value estimates using the experienced                as it does not require an oracle that knows the correct responses, and
transition from s to s′ with reward r:                                          is therefore a more realistic model of biological learning.
                                                                                   In task 1 (Fig. 1e,f and Extended Data Fig. 1a–c), in each episode, a dis-
                  V (s) ← V (s) + α[r + γV (s ′) − V (s)]                (6)    crete reward time tR is sampled between 1 and 15 and a discrete reward
                                                                                magnitude R sampled between 1 and 10. This defines a MDP shown in
where α is the learning rate. This update process converges to the values       Extended Data Fig. 1a. For this MDP, TD learning was used to learn the
defined above under very general conditions62.                                  value of the first state of the MDP s, which we refer to as the ‘cue’. In all
  After convergence, the value V(s) can be rewritten by taking the sum          tasks, the value of the cue was learned using one, two or three discount
and the discount factor outside of the expectation:                             factors (γ) from the set {0.6,0.9,0.99}, depending on the experimental
                                     ∞
                                                                                condition. The results indicated as ‘three discounts’ corresponds to
                          Vγ(s) = ∑ γ tE [rt|s ]                         (7)    the discount factors [0.6,0.9,0.99]. As there is noise in the simulation
                                    t =0                                        (see below), the results indicated as ‘one discount’ corresponds to the
                                                                                top performer over three identical discount factors ([0.6,0.6,0.6],
  Where we have added a γ subscript to the value to indicate that the           [0.9,0.9,0.9], [0.99,0.99,0.99]) and analogously for the results indicated
value is computed for that particular discount, and we have omitted             as ‘two discounts’. After performing TD learning, the values were fed as
the dependence of the expectation on π for simplicity. This last expres-        input into the policy gradient network whose output was the guessed
sion reveals a very useful property: Vγ(s), as a function of the discount       reward time (the network has 15 discrete actions, corresponding to
γ ∈ (0,1), is the unilateral z-transform of E [rt|s ] as a function of future   reporting reward times from 1 to 15). Performance was evaluated as
time t ∈ (0, ∞), of with real-valued parameter γ−1 (that is, the discrete-      the fraction of correct responses across test episodes (1 for estimating
time equivalent of the Laplace transform63). As the z-transform is invert-      the correct reward time, 0 otherwise). We have shown the performance
ible, in the limit of computing values with an infinite amount of γ, the        of the policy gradient network as it is trained in Extended Data Fig. 1c.
agent can recover the expected rewards at all future times {E [rt|s ]}∞t=0      In Extended Data Fig. 1m–o, we have shown a similar experiment but
from the set of learned values {Vγ(s)}γ∈(0,1):                                  using two reward times and reward magnitudes in the MDP. In this
                                                                                complex case, a more accurate decoding is obtained using five dis-
                     Z −1{Vγ(s)}γ∈(0,1) = {E [rt|s ]}t∞=0                (8)    counts instead of three.
                                                                                   In task 2 (Fig. 1e,f and Extended Data Fig. 1d–f), the structure of each
  Thus, if the agent performs TD learning with an infinite amount of            episode was as in task 1 but with a discrete reward time tR sampled
discounts, the converging points of the TD backups would encode not             between 1 and 8 and a discrete reward magnitude R sampled between
1 and 4. The learned values were input into a policy gradient network          in a branching task, as well as due to incomplete exploration of the
with 32 possible discrete outputs, representing the 32 possible hyper-         state space in a grid-world, and in a deep RL environment (see Methods
bolic values obtained in all the possible experiment (4 possible reward        below; Extended Data Fig. 2).
magnitudes × 8 possible reward times):                                            In tasks 1–4, the TD-learning process was corrupted by noise. In each
                                                                               episode, the learning rate was sampled from a normal distribution
                                         R                                     with mean of 0.1 and variance of 0.001 (denoted by N(0.1, 0.001)), the
                          V (s ) =                                     (10)
                                     1 + 0.9tR                                 number of TD backups was sampled from uniform(59,99) (except in
                                                                               the tasks with incomplete learning: tasks 3 and 4). This variability was
   Performance was evaluated as the fraction of correct responses              included to make sure that the decoder learns robust decoding strate-
across episodes.                                                               gies instead of just memorizing the exact values of each experimen-
   In task 3 (Fig. 1e,f and Extended Data Fig. 1g–i), we used the MDP          tal condition. For example, in task 1, with one discount, the value of a
shown in Extended Data Fig. 1g while keeping R fixed at 1 but varying          temporally close small reward was similar to the value of a temporally
tR and the number of times (N) that the full MDP had been experienced          far high reward, so reward time cannot be disentangled from reward
by the agents. As TD backups were performed online after every transi-         magnitude. However, although these two values were similar, they
tion, N is proportional to the total number of TD backups. The (possibly       were not identical, so a decoder with enough precision could learn to
incomplete) learned values at s from these N experiences were fed into         memorize them to report reward time. Introducing a small amount of
the policy gradient network (Extended Data Fig. 1h), which was trained         random noise in the learning process assures robustness in the evalu-
across episodes to optimize the reporting performance of tR.                   ation of the reporting performance.
   We also evaluated learning in incomplete-information situations                Finally, note that we used a non-linear decoder for the policy gradient
using the MDP shown in Extended Data Fig. 1p–r. In each episode, the           network instead of a linear one. As we showed in the previous section,
length of the two branches was uniformly sampled from 5 to 15 (if they         in principle, reward time can be decoded from value estimates with the
are the same, they were resampled until being different). Thus, in each        L−1 linear decoder. We used a non-linear decoder for two reasons. First,
episode, there is a shorter branch and a longer branch. Each branch            optimal performance in tasks 2 and 4 requires non-linear operations
was experienced a random number of times (N) sampled from a uni-               over the learned values. Task 2 requires computing the hyperbolic
form distribution with the range of 1–99 (denoted by uniform(1,99)).           value, and task 4 requires biasing towards myopic or far-sighted esti-
Thus, the number of TD backups performed for the two branches                  mates based on the estimated reward time. Second, even in tasks in
could be highly asymmetric. The learned values (with one or multiple           which the goal is only to report reward time (for example, tasks 1 and 3),
discounts) were fed as input into the policy gradient network with a           the linear decoder L−1 is only guaranteed to work well in optimal learn-
binary output indicating which path was the shortest one; performance          ing conditions (unnoisy value estimates with an infinite number of
was evaluated as the fraction of correct responses (Extended Data              discounts). In incomplete learning conditions, the L−1 decoder has been
Fig. 1o). Single-timescale agents can incorrectly believe that one branch      shown to be very sensible to noise21, which leads to poor performance
is shorter than the other branch if it has been experienced more often,        in the tasks studied here. In general, our goal in these simulations was
but multi-timescale agents can determine the distance to the reward            to illustrate the power of the multi-timescale representations over the
independently of the asymmetric experience. In the next section, we            single-timescale representations, as recoverable by a simple non-linear
present a theoretical proof showing that at any time during TD learn-          decoder.
ing (that is, before learning converges), multi-timescale systems can
perform the z-transform and decode the timing of non-zero rewards              Recovering temporal information before TD learning converges
(in the absence of timing stochasticity). In addition to the theoretical       In Extended Data Fig. 1s–w, we illustrate intuitively why the tem-
proof, we present an intuitive explanation (supported by Extended              poral information is available before TD learning converges for
Data Fig. 1s–w).                                                               multi-timescale agents (experiment in Fig. 1e). Consider the two
   In task 4 (Fig. 1e,f and Extended Data Fig. 1j–l), we keep the reward R     experiments in Extended Data Fig. 1s, one with a short wait between
fixed at 1 and tR varies between 1 and 4. Crucially, small random rewards      the cue and reward (pink) and one with a longer wait (cyan). For a
sampled from normal(0,0.25) were added to every state (fixed within            single-timescale agent (Extended Data Fig. 1t), the value of the cue
episodes). If the agent experienced the trajectory an infinite number          depends not only on the experiment length but also on the number
of times, the noisy rewards would be averaged out, so they would not           of times that each experiment has been experienced (N, the number
affect the value estimates of the cue. We note these ‘true’ value esti-        of TD backups). Thus, for a given set of learning parameters (learn-
mates as Vγtrue(s), to distinguish them from Vγ(s), which are the values       ing rate, discount factor, timestep length and reward magnitude),
learned with (incomplete) TD learning. In task 4, the agent experienced        the single-timescale agent can incorrectly believe that the cyan cue
the trajectory only once (that is, a single backup of TD learning along        indicates the shorter trajectory, if it has been experienced more often
the trajectory), so the small random rewards do affect the values Vγ(s)        (left part of the plot). However, as we show theoretically in this sec-
learned with TD learning. These noisy values are input into the policy         tion, as temporal information is encoded across discount factors
gradient network, the goal of which is to report the true value of the         for a multi-timescale agent, multi-timescale agents can determine
       true
cueVγ=0.9   (s), with a discount of 0.9, that would arise after experiencing   reward timing independently of N. In Extended Data Fig. 1u, the patterns
the trajectory of an infinite number of times (this is, ignoring the noisy     of three dots highlighted with rectangles are indicative of the reward
rewards). Although in task 4 we have illustrated the advantage of the          time and are only affected by the learning parameters by a multiplica-
myopic learning bias in a task in which the uncertainty on the value           tive factor. Indeed, when we plot the multi-timescale values as a func-
estimates arises due to stochastic rewards received at every state, the        tion of the number of times that the experiments are experienced (N;
myopic bias is beneficial independently of the origin of the uncertainty.      Extended Data Fig. 1v–w), we saw that the pattern across discounts is
For example, in more realistic state spaces, uncertainty usually arises        maintained, enabling a downstream system to robustly decode reward
due to incomplete exploration of the state space. In Extended Data             timing.
Fig. 2a, we illustrate that myopic estimates are generally more accurate          The following is a theoretical proof of this advantage. Consider a
when the near future is more certain than the far future, and far-sighted      multi-timescale agent performing TD learning on the trajectory
estimates are more accurate when the far future is more certain than           s → ⋯ → sT in which there is no variability in outcome timing (that is,
the near future. We have shown the benefits of the myopic learning             non-zero outcomes always happen at the same states, but their mag-
bias in more-realistic scenarios in which uncertainty arises due to noise      nitude can be stochastic) and all rewards are positive. Under these
Article
assumptions, the agent is able to decode reward timing if it has access                     Consider, for example, what happens if the agent has experienced
to {δ(rτ ,0)}Tτ =0, the future times at which outcomes rτ are non-zero given             the upwards and downwards trajectories (from s and s′) only once, as
the current state, where δ(rτ ,0) is a Kronecker delta function that is                  we show in Extended Data Fig. 2b. In this case, there are four possible
equal to 1 if rτ is zero and equal to 0 otherwise. At any time during TD                 scenarios depending on which specific trajectories the agent visits. As
learning, the value estimate for s computed with TD learning can be                      some of the possible paths are left unexplored, the agent must learn
written with the following general expression (note the absence of the                   from incomplete information, and its value estimates can differ from
expectation):                                                                            the those of an agent that has experienced all the paths. A perfect
                                                                                         agent that has experienced all trajectories will choose the upwards
                            T
                                                                                         trajectory from s and s′, and therefore the upwards trajectory is the
                  Vγ(s) = ∑ γ τfτ (α, N , R0:τ )(1 − δ(rτ ,0))                   (11)
                           τ =0                                                          ‘correct’ decision to make at s and s′. In Extended Data Fig. 2b, we show
                                                                                         that, when learning from incomplete information, the probability
where fτ (α, N , R0:τ ) is a non-zero scalar that depends on τ, on the learn-            of making the correct decision by following myopic estimates (low
ing rate α, on the number of times the trajectory has been experienced                   γ) versus far-sighted estimates (high γ) depends on the uncertainty
N and on the history of outcome magnitudes experienced in the past                       structure of the future. In s, in which the far future is more certain
R0:τ. This decoupling shares similarity with the successor representa-                   than the near future, it is beneficial to follow far-sighted estimates.
tion37,64,65. Crucially, fτ (α, N , R0:τ ) does not depend on γ, so, at all times        In s′, in which the near future is more certain than the far future, it is
during learning, it holds that:                                                          beneficial to follow myopic estimates. In summary, in s, the myopic
                                                                                         value estimates only integrate the certain near future, without being
            Z −1{Vγ(s)}γ∈(0,1) = { fτ (α, N , R0:τ )(1 − δ(rτ ,0))}Tτ =0         (12)    contaminated by the uncertain far future, leading to more accurate
                                                                                         value estimation. By contrast, in s′, the myopic estimates only see
   As fτ (α, N , R0:τ ) is non-zero for all τ and {1 − δ(rτ ,0)}Tτ =0 is only non-zero   the noisy near future, without being able to improve on this noise by
at τ in which a reward happens, the non-zero values of the right-hand                    the certain information that happens in the far future, leading to less
side expression indicates the future reward timings. In other words,                     accurate value estimation.
applying the inverse transform at any time during learning to the                           The myopic learning bias is independent of the source of the uncer-
multi-timescale estimate {Vγ(s)}γ∈(0,1) gives an expression whose                        tainty. In Extended Data Fig. 2a,b, the uncertainty comes due to incom-
non-zero values are the future outcome timings. In summary, in the                       plete state exploration (and also in the grid-world shown in Extended
absence of timing stochasticity, the multi-timescale agent can recover                   Data Fig. 2f–i; see Methods below). Conversely, in task 4 in Fig. 1 and
future outcome timing before TD converges, a capability that is not                      in the MDP shown in Extended Data Fig. 2c–e, the uncertainty comes
present in single-timescale agents.                                                      due to stochasticity in the rewards. Multi-timescale agents that have
                                                                                         myopic and far-sighted estimates at every state can, in principle,
The myopic learning bias                                                                 leverage this representation advantage to improve performance in
For the value estimate of a state s to converge to the expression shown                  specific tasks. For example, in task 4 and Extended Data Fig. 2c–e, the
in equation (1), learning needs to be ‘complete’, which requires that                    multi-timescale agent can determine the temporal distance to large
(1) all possible paths from s are explored; and (2) if there are stochastic              deterministic rewards (tR), and adapt accordingly between myopic or
rewards or transitions, all possible paths are explored a sufficiently                   far-sighted perspectives, leading to superior performance.
large number of times such that the stochasticity is averaged out.                          Note that, in general, exploiting the myopic learning bias requires
Although these two conditions are usually true in artificial laboratory                  two components: (1) having available myopic and far-sighted value
experiments, they are rarely true in natural environments. When these                    estimates and every state, and (2) knowing if the near future is more
two conditions are not met, value estimates must be computed on                          certain or uncertain than the far future at every state. The second com-
the basis of incomplete, uncertain information. The myopic learning                      ponent can be sometimes inferred by the multi-timescale values (such
bias states that, when learning from incomplete and uncertain infor-                     as in task 4), but this is not necessarily the case in general. For example,
mation, the accuracy of myopic versus far-sighted value estimates                        in Extended Data Fig. 2a,b, the second component might require the
depends on the uncertainty structure of the future. Myopic estimates                     agent to rely on separate uncertainty estimates, such as counting the
are more accurate if the near future is more certain than the far future,                fraction of paths left unexplored at different moments along the paths.
and far-sighted estimates are more accurate if the far future is more                    Another useful proxy to estimate the uncertainty structure of the future
certain than the near future.                                                            is the estimated distance to important environmental events (such as
   We illustrate the key idea of the myopic learning bias in Extended                    the distance to the landing zone in the Lunar Lander environment or
Data Fig. 2a,b. In Extended Data Fig. 2a, we show two states: s and s′.                  to the rewarded zone in the grid world, see below). In summary, only
In both cases, the animal must decide whether to take the upwards or                     multi-timescale learning systems satisfy component 1, which provides
downwards branch. The key difference between s and s′ is that, in s,                     a representational advantage that is absent in single-timescale systems.
the near future after the decision is more certain than the far future;                  This representational advantage can be exploited if the uncertainty
however, in s′, the far future after the decision is more certain than the               structure of the future is known. In some scenarios, the uncertainty
near future. These states represent frequent situations encountered                      structure of the future can be inferred by looking at the multi-timescale
in natural environments. For example, the two paths that leave from                      value array, but it could require separate uncertainty estimates in other
state s represent paths in which the far-away consequences are known                     scenarios.
with certainty, but there are multiple possible paths (with different and
unexplored outcomes) that eventually lead to the more certain states.                    Myopic learning bias: branching task. In Extended Data Fig. 2c, we
Conversely, state s′ represents a common exploratory situation, in                       present a simple MDP to highlight the advantages of the myopic learn-
which the branching-tree structure of the MDP causes the number of                       ing bias. In this maze, each state is associated with a random reward
possible outcomes to increase exponentially with the distance from s′.                   drawn from [–0.5,0.5], except for two states (s and s′; orange circles),
In incomplete learning scenarios, the structure of future uncertainty                    which result in a deterministic reward of 1. The optimal strategy in
from s and s′ is opposite. If the agent has not experienced all possi-                   this scenario is to move upwards at both states s and s′, which is the
ble trajectories from s and s′, in state s, the near future will be more                 policy that an optimal agent would implement after experiencing the
uncertain than the far future, and in state s′, the far future will be more              trajectories a sufficiently large number of times, after randomness is
uncertain than the near future.                                                          averaged out.
   However, in our simulation, the agent only learns from three trajec-      which we computed the true value function. We quantified that the
tories: (1) a trajectory that moves down at s, (2) another that moves up     myopic estimate is a better approximation of the true value function by
at s and up at state s′, and (3) a trajectory that moves up at s and down    evaluating the agreement between pairs of states along the estimated
at s′. As rewards are stochastic, the information that the agent gets on     and true curves. We evaluated accuracy using the Kendall rank correla-
each episode is incomplete. When learning from a limited number of           tion coefficient between the true value function in the entire maze and
experiences, the smaller stochastic rewards can overpower the larger         the value estimates. The Kendall coefficient measures the fraction of
deterministic rewards, making it challenging to achieve optimal perfor-      concordant pairs between the two value functions (across all pairs of
mance. At state s, only far-sighted agents can discern the significance      states in the maze). For every pair of states, it computes whether the
of the large deterministic rewards, thereby causing myopic agents to         two value functions agree on which element of the pair is the larger one.
perform near chance at s (Extended Data Fig. 2d, red). At state s′, the      Note that this measure of accuracy is behaviourally more relevant than
situation is reversed. Far-sighted agents not only integrate the close-by    alternative accuracy measures that compare the absolute magnitude of
large reward but also all the stochastic rewards farther in the future.      values across states. In other words, for an agent navigating the maze,
Myopic agents, in contrast, assign greater weight to the deterministic       it is more important to be accurate on the relative values of alternative
reward than to the future stochastic rewards, thus enabling optimal          goal states than on their absolute values.
performance at s′ (Extended Data Fig. 2d, blue). Therefore, only agents          In Extended Data Fig. 2h,i, the agent learned from N randomly sam-
that could dynamically adapt between being far-sighted at s and myopic       pled trajectories starting either in the lower half (blue) or upper half
at s′ can attain optimal performance when learning from limited experi-      (red) of the maze. The values for the states in the N sampled trajectories
ences (Extended Data Fig. 2e).                                               were learned until convergence using the rewards and transitions in
   To evaluate how well the agent acts given limited information, we         the sampled trajectories. After convergence, we computed the Kendall
averaged performance over the following procedure: (1) sampled               rank correlation between the estimates and the true value function,
rewards along the three trajectories mentioned before, (2) learned           and reported performance as the average correlation across 10,000
the Q-values (until convergence) for s and s′ using the rewards from         sets of N sampled trajectories. Extended Data Fig. 2h shows that when
the sampled trajectories, and (3) chose the actions that maximize the        learning from two randomly sampled trajectories, the estimates of
Q-values. In Extended Data Fig. 2d, we evaluated performance as the          the value function using a myopic discount factor are more accurate
fraction of episodes in which the Q-value of the branch with the deter-      than far-sighted discounts when the trajectories start in the lower half
ministic reward was higher than the Q-value of the branch without the        of the maze (blue curve in Extended Data Fig. 2h). This result agrees
deterministic rewards. Performance was measured as the proportion            with the intuition built in Extended Data Fig. 2g when learning from a
of correct decisions across 10,000 iterations of this procedure.             single trajectory. However, if the agent is distant from important events
   To evaluate the multi-timescale agent of Fig. 1d on this task (Extended   (that is, trajectories starting in the upper half of the maze, red curve),
Data Fig. 2e), we followed a similar procedure. In each episode, we ran-     the myopic estimates approach the noise level, whereas estimates
domized the identity of the top and bottom branches after the bifurca-       with larger discount factors are more accurate. With the accumula-
tion, which defines an episode-specific MDP. For each episode-specific       tion of more data from the environment, that is, more trajectories, the
MDP, the agent performed Q-learning until near convergence using the         far-sighted estimate progressively aligns with the true value computed
three trajectories mentioned in the previous paragraph. The Q-values         with γ = 0.99 in the entire maze (Extended Data Fig. 2i).
at the current state (s or s′) were fed into the policy learning architec-
ture of Fig. 1d, which outputs the decision to move up or down in the        Myopic learning bias: networks with discount factors as auxiliary
episode-specific MDP. The policy-learning network was trained across         tasks. An alternative way to leverage multi-timescale learning benefits,
episodes to produce actions that maximize overall task performance.          in contrast to the architecture presented in Fig. 1d, is to use them as
For the single-discount agent, we have reported the maximum perfor-          auxiliary tasks (Extended Data Fig. 2j). In this framework, the deep net-
mance over the agents with discounts [0.6,0.6] and [0.99,0.99], which        work acts according to the Q-value computed with a single behavioural
achieved a performance of 77 ± 2% and 83 ± 1%, respectively. For the         timescale, but concurrently learns about multiple other timescales as
multi-discount agent, we use the discounts [0.6,0.99], which achieved a      auxiliary tasks to enhance the representation in the hidden layers, which
performance of 94 ± 1%. The error bars correspond to the s.e.m. across       allows them to obtain superior performance in complex RL environ-
500 episodes in a validation set.                                            ments19,38,67,68. This approach is similar to distributional RL networks that
                                                                             learn the quantiles of the value distribution but act according to the
Myopic learning bias: grid world. Previous theoretical work showed           expectation of that distribution17. Of note, we showed that the auxiliary
that a myopic discount in RL can serve as a regularizer when approxi-        learning timescales display the myopic learning bias highlighted so far.
mating the value function from a limited number of trajectories66.           In the Lunar Lander task (Extended Data Fig. 2j, left) in which the agent
In Extended Data Fig. 2f–i, we highlight the fact that the benefit of the    must land a spacecraft, Q-values computed using a myopic discount
myopic discount is contingent on the distance between the current            provide a more accurate representation of the future when the agent
state and significant environmental events. Consider the simple naviga-      is close to the landing site (blue in Extended Data Fig. 2k), whereas
tion scenario depicted in Extended Data Fig. 2f. The motion of the agent     the opposite holds when the agent is far from the landing site (red in
is random and isotropic, garnering a minor random reward from a nor-         Extended Data Fig. 2k), as shown in Extended Data Fig. 2l.
mal distribution with mean 0 and s.d. 0.01 in each step and three more          In the Lunar Lander environment, the state space consists of eight
substantial rewards upon reaching the areas denoted by fire (r = –4) and     elements, including the position and velocity of the lander, its angular
water (r = 2) symbols. We evaluated how well the agent could determine       position and angular velocity, as well as an additional input related to
the true value function (under a discount factor γ = 0.99) under the         the contact with the ground. The action space is composed of four
aforementioned stochastic policy. Crucially, the agent performed this        actions: doing nothing and activating one of three different engines.
task after experiencing only a limited number of trajectories. The grey      The agent is a DQN3 with two hidden layers of 512 units each, separated
arrows show an example trajectory, with the actual and estimated values      by ReLU activation functions. In addition to the Q-values that control
for these trajectories shown in Extended Data Fig. 2g.                       the agent, the network has Q-values for 25 additional discounts factors
   Consider the trajectory shown in Extended Data Fig. 2g. For this          equally spaced between 0.6 and 0.99. Thus, if there are |a| actions in the
trajectory, the myopic estimate (using a discount factor γ = 0.6; green)     environment, for each discount the network has |a| additional output
clearly provides a better estimate of the true value function (grey) than    units. All sets of |a| units (one for each discount) use the Huber (that
using the discount factor γ = 0.99 (brown), which is the discount under      is, smooth L1, β = 1) Q-learning loss function with its corresponding
Article
discount. All the auxiliary Q-learning losses update the action that                   delivered. This reward delay cue was one of four possible odours, and
was actually chosen in the environment by the behavioural Q-value                      each cue was associated with a unique reward delay chosen from 0.6,
units, and thus all of them learn the consequences of the behavioural                  1.5, 3.75 or 9.375 s. The association between odour and reward delay
policy, but using different discount factors. The total loss function uset             was randomized across mice. The inter-trial interval was adjusted
to train the network averages the Q-learning losses of all the discount                depending on the reward delays such that the trial start cues were
factors. To train the DQN, we used a learning buffer of 20,000 samples,                spaced by 17–20 s. Mice performed 81.4 ± 12.5 trials (mean ± s.d.)
a learning rate of 10−3 and a batch size of 32. As in traditional DQNs, we             per session across the 36 sessions in which neurons were recorded in
used a target network to compute the TD target, which is updated every                 the task.
1,000 samples with the weights from the policy network to stabilize the
learning process. For exploration, the agent uses a linearly decreasing                Approach-to-target virtual reality task
ε-greedy policy that goes from ε = 1.0 at the first sample to a minimum                We refer the reader to the previous study for details on the experimental
value of ε = 0.01 after 40,000 samples.                                                procedures44. Mice were also trained in additional conditions, which
   Our goal was to compute the degree to which Q-values computed                       we did not analyse in the present study, including teleport and speed
with alternative discounts can capture the true Q-value of the behav-                  modulation in the virtual reality scene.
ioural policy. The multi-timescale DQN uses a behavioural discount                        Here we analysed single-neuron recordings in the sessions with no
γbeh = 0.99, and its policy is produced by choosing actions that maxi-                 teleport or speed manipulation and in the open-loop condition. Mice
mize the Q-values with that discount factor. As in the navigation sce-                 were free to locomote but their motion did not affect the dynamics of
nario presented in the previous section, our hypothesis was that, when                 the visual scene. After scene motion onset, the visual scene progressed
important events lie in the proximal future (here, close to the landing                at constant speed until reward was delivered after 7.35 s.
site), the Q-values learned using myopic discounts capture the true                       Mice performed 58.8 ± 21.7 trials (mean ± s.d.) per session across the
behavioural Q-value more accurately, whereas far-sighted discounts                     60 sessions in which neurons were recorded in the task. Spiking activity
are more accurate when important events lie in the distant future (far                 was convolved with a box filter of length 10 ms. When plotting neural
from the landing site).                                                                activity, we further convolved the responses by a causal exponential
   Under the policy of the DQN (πDQN), the true value of state s is:                   filter (e−0.05dt). Spiking-rate traces across neurons were normalized using
                                                                                       a modified z-score. The mean was taken as the average firing activity
                                            T                                        cross the first 1.5 s and the standard deviation across the entire 4.35 s.
                       Vγtrue
                          beh
                              ( s ) = EπDQN  ∑
                                            t =0
                                                     t
                                                   γbehrt 
                                                          
                                                                              (13)
                                                                                       Fitting neural activity in the cued delay task
                                                                                       For the cued delay task, we fit the responses of single neurons to the
   If the DQN has perfectly learned the Q-value of state s, then the esti-             delay cue (calculated as the firing rate in the time interval 0.1 s < t < 0.4 s
mate Qγ(s , abeh) of the DQN should be equal to Vγtrue       beh
                                                                 (s) , where abeh is   after the cue onset; see shaded area in Fig. 2c) using two discounting
the action produced by the DQN at s. For the analysis, we computed                     models as in ref. 63, the classic exponential model and a hyperbolic
the true value of state s by simulating the policy a large number of times.            model. For the exponential model, we fit the responses to a cue pre-
We evaluated accuracy as the degree to which the estimated Qγ(s , abeh)                dicting a reward in τ seconds by:
captures the trueVγtrue beh
                            (s), and compared accuracy across the auxiliary
discount factors.                                                                                            FRexp = b + αγ τ = b + αe −λτ                       (14)
   After training the network for 50,000 samples (and achieving
close-to-optimal performance), we computed Vγtrue              beh
                                                                   (s) empirically       The discount factor γ can also be expressed as a discount rate λ and
across states by recording the actual discounted sum of rewards                        vice versa: λ = −lnγ or γ = e−λ . The discount factors fitted to data are
obtained by the agent when departing from state s. We calculated                       always expressed in units of seconds, that is, the discount factor is the
Vγtrue
   beh
       (s) empirically for 25,000 states. Then, we compared, across states,            devaluation 1 s into the future.
the empirically calculated Vγtrue   beh
                                        (s) with the Q-values produced by the            For the hyperbolic model, we used a standard model for hyperbolic
DQN at those states.                                                                   discounting in which the parameter k controls discounting:
   To measure accuracy, we used the Kendall rank correlation as in the
previous section. The Kendall correlation measures the fraction of con-
                                                                                                                                    1
                                                                                                                 FRhyp = b + α                                   (15)
cordant pairs betweenVγtrue        and the estimated Qγ, across sampled pairs                                                    1 + kτ
                               beh
of states. As in the navigation scenario presented in the previous section,               We fit both models by minimizing the mean squared error (the fit
for an agent deciding which state to navigate to, it is more important                 function in MATLAB). For both models, we constrained the baseline
to be accurate on the relative values between pairs of states than on the              and gain parameters such that 0 < b < 40 and 0 < α < 40. For the expo-
absolute value of individual states. Therefore, the Kendall correlation                nential model, the discount rate was constrained such that
is behaviourally more relevant than other accuracy metrics that com-                   0.0001 < λ < 20, and for the hyperbolic model, the discount parameter
pare the absolute magnitude (for example, (Vγtrue                               2
                                                        beh(s ) − Qγ (s , abeh)) ).    was constrained such that 0 < k < 20. Note that all the parameters were
   Given that the environment and the training process are stochastic,                 fitted independently for each single neuron.
we reported the accuracy by averaging over 10 randomly initialized                        To characterize the robustness and significance of our estimated
networks.                                                                              parameters, we used a bootstrap procedure. For each run, we split the
                                                                                       trials in half and fit the models independently on each half. We com-
Cued delay task                                                                        puted for each split the explained variance using the other half of the
All the data in the experiments with mice were collected in the previous               data (Extended Data Fig. 3c,d) and correlated the inferred parameter
study44. The experimental details, including the surgical procedures,                  values for each neuron across both splits (Extended Data Fig. 3f–h).
behavioural setup and the behavioural tasks, have been described                          We restricted our subsequent analysis to neurons that had a positive
there44. Here we focus on the task description as our analysis includes                explained variance on the test set (n = 17 neurons excluded), an average
task conditions that were not analysed in the previous study.                          firing rate in the cue period over the 4 delays above 2 spikes per sec-
  Mice were head-fixed on a wheel in front of three computer moni-                     ond (n = 11 neurons excluded) and with mediolateral distance above
tors and an odour port. At trial onset, the screens flashed green to                   900 µm (n = 4 neurons excluded). Non-selected neurons are shown
indicate the beginning of the trial. After t = 1.25 s, an odour cue was                in Extended Data Fig. 3b. Poorly fit neurons often were non-canonical
dopaminergic neurons that also did not exhibit a strong reward                                                          L
                                                                                                                            σ 2  uT ∆ v
response.                                                                                                    E (r|t) ≈ ∑  2 s 2  s d s ≡ L−1∆d                 (22)
                                                                                                                       s=1  α + σs  σs
Decoding expected reward timing from population responses
The vectorized prediction error allows us to directly decode the                              where ∆d = [δ1…δN ]T . The smooth regularization introduced by the
expected timing of reward given the cue responses21. The value at                             Tikhonov regularization through the parameter α (which we can choose
time t is given by:                                                                           by inspection of the distribution of singular values σs, see below) is
                                                                                              more robust than a strict truncated SVD in which we only take a number
             T                                                                              of factors and set the remaining factors to zero. An alternative approx-
      Vt = E ∑ γ trt  = γ ∆tE (r|∆t) + γ 2∆tE (r|2∆t) + … + γT E (r|T )            (16)
              t                                                                           imation to this inverse problem is Post’s approximation34,37. It relies on
                                                                                              evaluating higher-order derivatives and lacks robustness if the Laplace
   In the cued delay task, at the time of the cue indicating reward                           space is not sampled with enough precision (that is, not enough neurons
delay, the response of dopaminergic neurons is driven by the dis-                             tiling the γ space).
counted future reward. The reward prediction error δt = rt + γ ∆tVt +1 − Vt                      The procedure in the previous section allows us to estimate the dis-
becomes simply δt = γ ∆tVt +1 + cst as there is no reward delivered at                        count factor independently for each neuron. We then choose a discre-
the time of the cue (rt cue = 0) and the reward expectation before the                        tization step Δt = 100 ms and a temporal horizon T = 12 s over which to
reward cue delivery is identical across conditions (Vt cue = C ; where C                      make the prediction. This allowed us to construct the discount matrix
is a constant). Thus, the TD error at the time of reward delay cue                            L shown in Fig. 2i for the exponential model and Extended Data Fig. 6c
(δt cue = rt cue + γ ∆tVt cue+∆ t − Vt cue) becomes δt cue = γ ∆tVt cue+∆ t + C , and if we   for the hyperbolic model. To choose a suitable value for the regulari-
assume the constant is 0 or the TD error is baseline subtracted, at con-                      zation parameter α, we performed the regular SVD on the discount
vergence the prediction error is given by:                                                    matrix L and assessed the values at which the singular values become
                                                                                              negligible. We chose a value of α that corresponded to the transition
                                                            E (r|∆t)                        between large singular values and negligible values (Extended Data
                                                                     
                                                       T   E (r|2∆t)                       Fig. 4b). Using this approach, we used α = 2 in our decoding analysis.
                    δi = γi   ∆t
                                     γi   2∆t
                                                 … γi                               (17)
                                                            ⋮                                  For each delay, we constructed a pseudo-population response Δd
                                                         E (r|T )                           across the recorded neurons. For each bootstrap, we took the mean
                                                                  
                                                                                              activity for each cue, subtracted the inferred baseline parameter b and
    In single-timescale RL, the temporal information is collapsed, and                        normalized the maximum response to 1. To assess the robustness of
it is not possible for the system receiving the learning signal (the stria-                   the predictions, we used the mean responses and baseline from half
tum in this case) to untangle the signal. However, in a distributed system                    the trials to construct Δd, used the estimated discount factors from the
learning at multiple timescales, the reward expectation E (r|t)is encoded                     other half of the trials to estimate L−1 and we repeated this approach for
with multiple discount factors γi:                                                            each bootstrap (npredictions = 200). In the figures (Fig. 2k and Extended
                                                                                              Data Figs. 5g, 6d,f and 10c), the thin lines correspond to the predictions
                      γ ∆t γ 2∆t             … γ1T   E (r|∆t)                             from individual bootstraps and the thicker line to the average of these
                        1    1
              δ1   ∆t 2∆t                         
                                                                                            predictions. For shuffle control, we randomized the identity of the
                γ2 γ2                      ⋯ γ2T   E (r|2∆t)
             ⋮= ⋮          ⋮               ⋱ ⋮
                                                     
                                                             ⋮    
                                                                    = Lp(r|t)        (18)     neurons in the pseudo-population response Δd. This means that in the
             δn                                                                        shuffle control, a given neuron is not decoded with its corresponding
                      γ ∆t γ 2∆t             ⋯ γnT   E (r|T ) 
                      n     n                                                                weights but by a random row of the decoding matrix L−1.
                                                                                                 To ensure that the prediction corresponded to a probability distribu-
  The temporal information about reward timing is now distributed                             tion, we normalized the resulting prediction of reward timing. We first
across neurons, and if the tuning of individual neurons is sufficiently                       set the probability of obtaining a reward to zero for all times in which
diverse, we can write:                                                                        the prediction was negative, then we normalized the distribution to
                                                                                              be a valid probability distribution (such that the probability mass over
                                 E (r|∆t)                                                   t ∈ [0,12] summed to 1).
                                                  δ1 
                                 E (r|2∆t) ≈ L−1  ⋮                              (19)
                                                                                                 For the time decoding using a single average discount factor, we
                                      ⋮                                                   used a different approach. The inversion procedure would not work
                                 E (r|T )         δn 
                                                                                            as the discount matrix would be of rank 1. Instead, if we assume a fixed
                                                                                              known reward size and a single discount factor, the response of indi-
   Where L−1 is the approximate pseudo-inverse of L such that L−1L ≈ I .                      vidual neurons would correspond to different estimates of the reward
In practice, the matrix L is not very well conditioned as the rows of                         timing. For each bootstrap, we estimated the expected reward timing
the matrix are exponentially decaying functions, so the right side                            for each neuron. For a given firing rate FR for the held-out data, we
(further in the future) is sparsely populated (Fig. 2i and Extended Data                      estimated the reward timing using the parameter estimates from the
Fig. 4a–d). We therefore need to use a regularized pseudo-inverse.                            trained data. The baseline bi and gain αi parameters are specific to each
   To invert the discount matrix L, we used the regularized singular                          neuron, whereas the discount factor γ is the average discount factor
value decomposition (SVD) approach similar to the one proposed in                             across all the neurons. The expected reward timing for neuron i is given
ref. 21. We then normalized the resulting prediction to constrain it to                       by the following equation:
be a probability distribution ( p(r|t) > 0, for all t and ∑t p(r|t) = 1). More
specifically, the regularized SVD approach corresponds to optimizing:
                                                                                                               Ei(t) =
                                                                                                                         log max   (   FR i − bi
                                                                                                                                          αi
                                                                                                                                                 , 0.0001   )      (23)
                                                  2    2          2
                                                                                    (20)                                               logγ
                       ||Lp(r|t) − ∆d || + α ||E (r|t)||
                                                                                                Together, the neurons provide a distribution of expected reward tim-
   The standard SVD of the discount matrix can be written as:                                 ing with each neuron predicting a sample of the distribution of expected
                                          L                                                   reward times. The average distribution is obtained by averaging the
                                L = ∑ σsusvsT = USV T                                (21)     distributions across all the bootstraps, excluding predicted reward
                                     s=1                                                      times beyond 12 s and normalizing the distribution to be a probability
Article
distribution. Similarly to the SVD-based decoding, in Extended Data              We compared these modulation indices to the inferred discount
Fig. 4f, the thin lines correspond to the predictions from individual          factors, showing no significant correlations, whereas the three meas-
bootstraps and the thicker line to the average of these predictions.           ures of licking are strongly correlated among themselves (Extended
                                                                               Data Fig. 8).
Quantifying reward timing decoding accuracy
To quantify the reward timing decoding accuracy, we used the                   Fitting neural activity in the virtual reality task
1-Wasserstein distance (or earth mover’s distance) between distribu-           To quantify the heterogeneity of discount factors in the virtual reality
tions as our metric. We used the 1-Wasserstein distance as the differ-         task, we fit the neural activity in the last 4.30 s (t = 3.05 s after scene
ence in support between the predicted reward timing distribution               motion onset) of the approach to reward period in which the ramping
(probability mass as most locations) and the single true reward timing         activity was most pronounced. To assess the robustness of the fit, we
(probability mass at a single location) is not conducive to using the          used a bootstrap procedure in which for each bootstrap (nbootstrap = 100),
Kullback–Leibler divergence.                                                   we partitioned the trials into two halves and computed the two average
   For each bootstrap, we generated n = 100,000 samples from the               PSTHs using dt = 0.1 s as our discretization step. We then computed
predicted reward timing distributions and computed the 1-Wassertstein          the mean value of the parameters across all bootstraps. We limit our
distance between the predicted reward timing and the true correspond-          analysis to neurons whose firing rate over the analysis period is larger
ing reward delay (using the MATLAB function ws_distance from https://          than 2 spikes per second. We fit the two models (common value func-
github.com/nklb/wasserstein-distance). For each condition (expo-               tion and common reward timing expectation) to this data.
nential fit, hyperbolic fit, average discount factor, simulation fit and          In the virtual reality task, the expectations vary smoothly as a func-
their associated shuffled predictions), we obtained a distribution of          tion of time and distance and we therefore use the discretized formula-
1-Wasserstein distances across the bootstraps (n = 200). To assess the         tion of the TD error for continuous time in our fits45,47:
significance of the differences in reward timing predictions across
conditions, we used the one-tailed Wilcoxon’s signed-rank test (using                                         dV ( t )                    
                                                                                             δi(t) = bi + αi γi dt     − γi dt ln(γi)V (t)         (25)
the MATLAB function signrank).                                                                                     dt                     

Analysing behavioural discounting through the lick rate in the                   Although this formulation is also discretized as the standard formu-
                                                                                                                                         dV (t )
cued delay task                                                                lation of the TD error, the presence of the derivative i (which is
                                                                                                                                          dt
To quantify the influence of behaviour on the discount factors inferred at     computed numerically) improves the stability of the fitting procedure.
the single-neuron level, we analysed the relationship between the behav-       The two models differ in whether value function is estimated directly
ioural discounting and the neural discounting. To quantify behavioural         (and shared across neurons) or indirectly (and distinct across neurons).
discounting, we used the anticipatory lick rate in the 0.6 s following the     The discount factor is also in units of seconds, allowing comparison
delay cue. For each neuron, we computed the average lick rate for each of      with the values estimated in the cued delay task.
the four delays. We then fitted an exponential model (as for the neurons)
to the four average lick rates for the four delays (shown in Extended Data     Common value function model
Fig. 5a,d, left panels). For each neuron, we therefore obtained a behav-       In the common value function model, V(t) is common across neurons
ioural discount factor. To quantify the relationship between behavioural       and is directly fitted by the optimization procedure which minimizes:
and neural discount factor, we used the Spearman rank correlation.
                                                                                                        minα , b, γ, V ∣∣FR − ∆∣∣2                   (26)
Comparing the neural discount factor as a function of lick rate
To investigate the effect of the of the lick rate on the discount factor         With,
measured in single neurons, we also compared the modulation of the
                                                                                                          δ1(t0) ⋯ δ1(T ) 
inferred discount factor with the lick rate. For each neuron and each                                                       
                                                                                                      ∆=  ⋮       ⋱   ⋮                            (27)
reward delay, we split the trials into low and high lick rate trials depend-
ing on whether the lick rate in the entire anticipation period was strictly                               δn(t0) ⋯ δN (T )
below or above (or equal) to the median lick rate for this reward delay.
We then fit the exponential discounting model separately for low lick             We fit the gains, baseline, and discount factors of individual neurons
rate trials and high lick rate trials. We compared the difference in the       (αi , bi and γi respectively) and the join value function V using a con-
inferred discount factor for each neuron across the two conditions. We         strained optimization procedure (fmincon in MATLAB, αi ∈ [0.05,50],
performed this analysis at the single-animal level for the two animals         bi ∈ [0.05,12], γi ∈ [0.05,0.999999], and V ∈ [0.05,5]).
with sufficient number of neurons (Extended Data Fig. 5h,i).                      We performed this analysis both on the full population of neurons
                                                                               that passed the inclusion criteria (Fig. 3) as we well as independently for
Analysing behavioural discounting through the lick rate in the                 the subsets of neurons belonging to m3044 (29 neurons) and m3054
virtual reality task                                                           (24 neurons; Extended Data Fig. 8e–j).
To quantify the relationship between behaviour and neural activity
in the virtual reality task as mice approach the reward location, we           Common reward expectation model
computed a measure of ramping in the lick rate and compared it with            In the common reward expectation model, the reduction in uncer-
our measure of neural discounting. For each neuron, we computed the            tainty in reward timing due to sensory feedback as the mice approach
average lick rate during the reward anticipation period as the mice are        the reward leads to an upwards ramp in the average TD error signal
moving along the linear track. We computed the average lick rate in            across dopaminergic neurons44–46. In a task such as the cued delay task
three windows: early (−3.45 s to −2.95 s before reward delivery), middle       shown in Fig. 2, once the cue has been presented, the time estimation
(−1.95 s to −1.45 s before reward delivery) and late (−0.75 s to −0.25 s       until the reward is based on the internal clock of the mice that experi-
before reward delivery). Using these windows, we computed three                ences scalar timing (that is, the standard deviation of the noise in the
modulation indices using the following equation:                               estimation grows linearly with the estimation time)26. In the virtual
                                                                               reality task, there is visual feedback, and as the mice approach the
                             Lick rate2 − Lick rate1                           reward, the uncertainty is instead reduced (Extended Data Fig. 9a).
                   MI2−1 =                                             (24)
                             Lick rate2 + Lick rate1                           We also showed that this alternative model also provides a similar
explanation of ramping diversity as originating from a heterogeneity            one with a common value function across all neurons and another
of discount factors.                                                            with a common reward timing estimation; whereas in the cued
   We use a joint fitting procedure in which we simultaneously fit the          delay task, each dopaminergic neuron contributed to learning an
discount factors across neurons and the expected timing of reward as            independent value function. Here we reconcile these approaches
a function of position in the virtual track. Similarly to ref. 45, we inter-    as shown in Extended Data Fig. 7f–k. We can understand these dif-
pret the ramping in single neurons as originating from the reduction            ferent models as a spectrum in which the value functions used for
in uncertainty due to the visual feedback as the mice approach the              the computations for each discount factor is more or less shared
reward. Although each neuron has a distinct discount factor and its             across them.
own value function, the world model, which parametrizes the changes                In ‘classic’ multi-timescale TD learning19, the values (Vi) and RPEs (δi)
in reward expectation with visual feedback, is shared across dopamin-           for the different discounts (γi) are updated independently, which guar-
ergic neurons. This arises as this shared model is the product of the           antees its convergence. Now consider a situation in which the value–RPE
integration of the diverse dopamine signals, as well as of other neural         circuits of the multiple discounts share a common value function, to
computations that control reward expectations69.                                a degree λ between 0 and 1. In this case, the next value estimate in the
   Individual neurons therefore act as independent agents estimating            timescale-independent TD backup is corrected by:
value given a shared expectation of reward timing. Each neuron has                                                        ∼
a distinct discount factor γi with which it computes value given the                           Vi(st) ← Vi(st) + α(rt + γV  (s ) − Vi(st))
                                                                                                                         i i t +1
                                                                                                                                                       (32)
expected reward timing. We assumed that inference has converged
and therefore we have the value Vi associated with neuron i:                                              ∼     ∼
                                                                                                          Vi = λV + (1 − λ)Vi                          (33)
                                 T                                                         ∼
                                                                                   Where V is the mean value function across all discounts. The main
                           Vi = ∑ γi τ − tE (r|τ , t , T )               (28)
                                τ= t                                            motivation for this modification of the traditional TD backup is neu-
                                                                                roanatomical, as it is plausible to consider a degree of common shared
   Here we assumed that E (r|τ , t , T ) takes the form a folded normal         activity across nearby value units. Unlike the model with a fully shared
distribution with parameters μ = T − t and (fitted) standard deviation          common value function (that is, λ = 1), multi-timescale learning with
σ. The folded normal distribution reflects the weight of the negative           small values of the sharing parameter (for example, λ = 0.1) preserve,
component of a normal distribution back onto positive values70. The             to a large degree, all the computational advantages of multi-timescale
folded normal distribution formulation leads to the following distribu-         learning, while being more biologically realistic than fully separated
tion for the expected reward timing for τ > 0:                                  circuits (that is, λ = 0). Using λ = 0.1 in the four tasks of Fig. 1e (while
                                                                                keeping the same simulation parameters as in Fig. 1; see ‘Decoding
                                           2       2
                                                                                tasks’ in Methods), the accuracy of the report of the agent with three
                                2 − (τ +(T 2− t ) )       (T − t )τ 
          E (r|τ , t , T ) =        e   2σ          cosh               (29)   discounts {0.6,0.9,0.99} is 82 ± 5% in task 1, 94 ± 2% in task 2, 92 ± 3% in
                               πσ 2                       σ
                                                               2
                                                                               task 3 and 93 ± 3% in task 4. Therefore, regularizing the independent
                                                                                value functions with a small degree of shared activity preserves all the
   In our analysis, the mean, μ = T − t , is given by the current position      multi-timescale advantages highlighted so far.
in the virtual reality track and the only fitted parameter is the standard         With this modification to the learning rule, Vi does not converge to
deviation σ. At each timestep, we fit a different value of the standard         a pure exponential form anymore (compare dashed lines with solid
deviation. As observed through the fitting procedure, the standard              lines in Extended Data Fig. 7f–h value panels, middle row), even with a
deviation was initially high and reduced as the mice approached the             small sharing parameter ( λ = 0.1 in Extended Data Fig. 7g). As a result,
reward location. This is an indication that similarly than proposed in          the RPE does not converge to 0 across the trajectory (Extended Data
ref. 45, the ramping in activity in the dopaminergic neuron arises from         Fig. 7g,h, δ, bottom row), so TD learning does not fully converge at the
the reduction in uncertainty due to the visual feedback as the mice             level of individual value estimators. However, we found empirically
approach the reward. We used a slightly different formulation than in           that learning stabilizes completely after 1,000 TD backups (using
ref. 45 as we required additional flexibility to fit data and specifically      α = 0.1). Crucially, owing to the non-exponential form of the learned
needed to go beyond the assumptions of Gaussian state uncertainty.              value function, we observed that γ < γ∼(γ > γ∼) have RPEs that mostly
Note also that here we assumed that the uncertainty is in the timing of         ramp down (or up), so our characterization of cell-specific discounts
the reward rather than in the state.                                            based on ramping patterns is mostly independent of which of the ramp-
   To normalize the contributions of the different neurons, we used a           ing interpretations we adopt. These ramping patterns across timescales
normalized firing rate and therefore only fit the discount factor γ and         are robust when varying the magnitude of the sharing parameter
standard deviation σ of the reward expectation.                                 λ (λ ∈ [0,1]), the learning rate α and the number of TD backups (we used
                                                                                a learning rate α of 0.01 and 2,000 TD backups for Extended Data
                               minγ, σ ∣∣FR − ∆∣∣2                       (30)   Fig. 7f–h). For the simulations in Extended Data Fig. 7f–h, we used the
                                                                                discounts fitted experimentally in Fig. 3l (90 units), and plot only three
  With,                                                                         discounts in Extended Data Fig. 7f–h (renormalized to lie between –1
                                                                                and 1) corresponding to the 20th, 70th and 90th percentiles, corre-
                               δ1(t0) ⋯ δ1(T )                                sponding to discounts 0.25, 0.56 and 0.88.
                                                
                           ∆=  ⋮      ⋱   ⋮                            (31)      The model in the cued delay task corresponds to λ = 0. The common
                              δn(t0) ⋯ δN (T )                              value function model that we propose would be similar to a value of
                                                                                λ = 1, but note that the common value model used for fitting in equa-
   We performed the constrained optimization with the MATLAB func-              tion (25) is slightly different than in equation (32), as in equation (32)
tion fmincon and constrained the parameters such that γ ∈ [0.001,0.99]          the shared value is only used for estimating future value. The model
and σ ∈ [0.1,12].                                                               from equation (32) corresponds more closely to the common reward
                                                                                timing estimation model (Extended Data Fig. 9) in which the reduction
Mixing in distributed RL models                                                 in uncertainty with visual feedback affects the estimate of future value
When explaining how multi-timescale RL can explain the diversity                as outlined below. In this model, the TD error for neuron i can be writ-
of ramping activity, we proposed two possible interpretations:                  ten as follow: δi = γV  ′ − Vi.
                                                                                                      i i
Article
          T                             T
    Vi = ∑ τ = t γi τ − tE (r|τ , t , T ) = ∑ τ = t γi τ − tEτ is the value before the sen­­             For the virtual reality task, for each neuron, we generated n = 80 trials,
                                           T                                        T
sory feedback and V ′i = ∑ τ = t +1 γi τ − t −1E ′(r|τ , t + 1, T ) = ∑ τ = t +1 γi τ − t −1E ′ τ is   comparable with behavioural sessions in the task, by taking samples
the value of the next state, including the sensory feedback                                            from a Poisson distribution with a rate parameter corresponding to the
and the reduction in uncertainty in reward timing. To highlight                                        predicted activity given equation (22). We then performed the fitting
the contribution of the sensory feedback, we also introduced                                           procedures similarly than for the experimental data.
          T                                               T
Vit +1 = ∑ τ = t +1 γi τ − t −1E (r|τ , t + 1, T ) = ∑ τ = t +1 γi τ − t −1Eτ , which would be the
value at the next step in the absence of sensory feedback (and therefore                               Reporting summary
no reduction in uncertainty about reward timing).                                                      Further information on research design is available in the Nature Port-
   We can rewrite the TD error as follows:                                                             folio Reporting Summary linked to this article.
                                              t +1      t +1
                   δi = γV ′ − Vi = γV
                         i i
                                       ′ − γV
                                     i i    i
                                                   + γV
                                                      i
                                                             − Vi
                                                       i           i
                                        t +1
                                                                                             (34)      Data availability
                                δi = γV
                                      i
                                             − Vi + γi ∆Vi
                                            i                                                          The raw electrophysiological data can be found on DANDI Archive
   Here the correction due to the sensory feedback appears as ΔVi,                                     (https://dandiarchive.org/dandiset/000251). The curated electrophysi-
                                  T
which we can also write as ∑ τ = t +1 γi τ − t −1(E ′ τ − Eτ) . Similarly than in                      ological data can be found at https://doi.org/10.17632/tc43t3s7c5.1
ref. 45, the sensory feedback acts as a correction term in the prediction                              (see ref. 72).
error computation. Here the shared correction term is the reduction
in uncertainty, so it takes a slightly different form than in the general
                                    ∼                                                                  Code availability
formulation with shared value V and would correspond to a case in
which the sharing parameter depends on the discount factor. This                                       The code used for simulations can be found on GitHub (https://github.
source of ‘regularization’ could occur through different pathways.                                     com/pablotano8/multi_timescale_RL). The data analysis code for
Here it is the reduction in uncertainty due to the structure of the virtual                            the electrophysiological experiments can be found at https://doi.
reality task that leads to a reduction in the uncertainty about reward                                 org/10.17632/tc43t3s7c5.1 (see ref. 72).
timing as the mice approach the reward. This contribution of a shared
signal or estimate from a parallel value estimation has also been used to
explain non-canonical prediction errors in motor tasks71. In the cued                                  61.   Kvitsiani, D. et al. Distinct behavioural and network correlates of two interneuron types in
                                                                                                             prefrontal cortex. Nature 498, 363–366 (2013).
delay task, there is no feedback about reward timing and therefore we                                  62.   Sutton, R. S. Learning to predict by the methods of temporal differences. Mach. Learn. 3,
have ΔVi = 0 in equation (34), and this would correspond to a situation                                      9–44 (1988).
in which the loops are entirely decoupled (λ = 0). In practice, we might                               63.   Oppenheim, A., Willsky, A. & Hamid, W. Signals and Systems (Pearson, 1996).
                                                                                                       64.   Dayan, P. Improving generalisation for temporal difference learning: the successor
expect some low level of coupling given the anatomical considerations                                        representation. Neural Comput. 5, 613–624 (1993).
outlined above. As long as the loops are not completely coupled (λ ≠ 1),                               65.   Gershman, S. J. The successor representation: its computational logic and neural
there is enough information to leverage the computational advantages                                         substrates. J. Neurosci. https://doi.org/10.1523/JNEUROSCI.0151-18.2018 (2018).
                                                                                                       66.   Amit, R., Meir, R. & Ciosek, K. Discount factor as a regularizer in reinforcement learning.
shown in Fig. 1 and perform the decoding shown in Fig. 2 (Extended                                           In Proceedings of the 37th International Conference on Machine Learning 269–278 (PMLR,
Data Fig. 7i–k).                                                                                             2020).
                                                                                                       67.   Badia, A. P. et al. Agent57: outperforming the Atari human benchmark. In Proceedings of
                                                                                                             the 37th International Conference on Machine Learning 507–517 (PMLR, 2020).
Comparing parameters across tasks
                                                                                                       68.   Reinke, C. Time adaptive reinforcement learning. ICLR 2020 work. Beyond tabula rasa RL.
We used two methods to assess the relationship between the inferred                                          Preprint at https://doi.org/10.48550/arXiv.2004.08600 (2020).
discount factors in the approach-to-reward virtual reality task and the                                69.   Gershman, S. J. & Uchida, N. Believing in dopamine. Nat. Rev. Neurosci. 20, 703–714
                                                                                                             (2019).
cued delay task. First, we used the mean parameters across bootstraps                                  70.   Leone, F. C., Nelson, L. S. & Nottingham, R. B. The folded normal distribution. Technometrics
and computed the Spearman rank correlation. Next, we computed, for                                           3, 543–550 (1961).
n = 10,000 randomly selected (with replacement) pairs of bootstraps,                                   71.   Lindsey, J. & Litwin-Kumar, A. Action-modulated midbrain dopamine activity arises from
                                                                                                             distributed control policies. Adv. Neural Inf. Process. Syst. 35, 5535–5548 (2022).
the Spearman rank correlation between the parameters across the two                                    72.   Masset, P. et al. Data and code for ‘Multi-timescale reinforcement learning in the brain’,
tasks and plotted the distribution of these correlation.                                                     V1. Mendeley Data https://doi.org/10.17632/tc43t3s7c5.1 (2025).
   For the decoding of reward timing using parameters inferred in the
virtual reality task, we also used a bootstrap approach. We computed                                   Acknowledgements We thank S. J. Gershman and J. Mikhael for their contributions to the
the discount matrix and the decoding matrix for each bootstrap esti-                                   preceding studies; M. Watabe-Uchida for advice on task design; members of the Uchida and
                                                                                                       Pouget laboratories, including A. Lowet and M. Burrell, for discussions and comments; and
mate of the discount factors in the virtual reality task.                                              W. Carvalho, G. Reddy and T. Ott for their comments on the manuscript. This work is supported
                                                                                                       by US National Institutes of Health (NIH) BRAIN Initiative grants (R01NS226753 and U19NS113201),
Simulations to assess limits on parameter estimation                                                   NIH grant 5R01DC017311 to N.U., and a grant from the Swiss National Science Foundation
                                                                                                       (315230_197296) to A.P. This research was carried out in part thanks to funding from the
To assess the contribution of the limits imposed by the number of tri-                                 Canada First Research Excellence Fund, awarded to P.M. through the Healthy Brains, Healthy
als and the stochasticity in firing rates to the accuracy of the reward                                Lives initiative at McGill University.
timing prediction and the similarity of inferred parameters across
                                                                                                       Author contributions P.M., P.T., A.P. and N.U. conceived the project. P.M., H.R.K., A.N.M.
tasks, we ran a series of simulations with parameters chosen to match                                  and N.U. designed the electrophysiology experiments. A.N.M. and H.R.K. performed the
those inferred from the data. For the simulation parameters, we used                                   electrophysiology experiments and curated the data. P.T. performed the simulations with
the mean inferred value for the parameters across all the bootstraps                                   artificial agents. P.M. performed the analysis of electrophysiological data. P.M., P.T., A.P. and
                                                                                                       N.U. wrote the paper with input from H.R.K.
for the respective task.
   For the cued delay task, for each neuron, we generated n = 80 trials                                Competing interests The authors declare no competing interests.
(n = 20 per delay), comparable with behavioural sessions in the task,
simulated cue responses by taking samples from a Poisson distribu-                                     Additional information
                                                                                                       Supplementary information The online version contains supplementary material available at
tion with a rate parameter corresponding to the value predicted by                                     https://doi.org/10.1038/s41586-025-08929-9.
the exponential discount model for the corresponding reward delay.                                     Correspondence and requests for materials should be addressed to Paul Masset,
We used the same procedure as for analysing the recorded data by per-                                  Alexandre Pouget or Naoshige Uchida.
                                                                                                       Peer review information Nature thanks Kenji Doya and the other, anonymous, reviewer(s) for
forming n = 100 bootstraps and fitting the simulated data on random                                    their contribution to the peer review of this work. Peer reviewer reports are available.
partitions of the data.                                                                                Reprints and permissions information is available at http://www.nature.com/reprints.
            a                                                                              d                                                                                     g

                              0    0          0          0                                                    0   0                          0              0                                                           0       0          0     1     0


            b                                                                              e                                                                                     h

                           Sample        TD-learning       Report                                                     TD-learning                                    Report                    Sample                                 TD-learning         Report
                                                                                                    Sample
                           episode                       reward time                                episode                                                         hyperbolic                 episode                                (N iterations)    reward time
                          parameters                                                               parameters                                                       value of s                parameters

                c    1                                                                     f        1
                                                                                                                                                                                 i         1




                    0.5                                                                            0.5
                                                                                                                                                                                          0.5




                     0                                                                              0
                                                                                                                                                                                           0
                          0       500        1000        1500                                             0           500                        1000           1500                                            0                   500        1000        1500


                j                                                                              m                                                                                     p                                                Du
                                                                                                                                       1                                2                                                                              R=1

                                                                                                                                                                                                                                Dd
                                                                                                          0           0             1            0              0
                                                                                                                                                                       2    0
                                                                                                                                                                                     q                                                     R=1
                k                                                                              n


                                                                                                                          TD-learning                             Report                  Sample                                       TD-learning          Report shorter
                           Sample        TD-learning                         Report true            Sample                                                     reward times
                                                                                                    episode                                                                               episode                                     (N iterations)        branch
                           episode       (1 iteration)                        value of s                                                                                                 parameters
                          parameters                                                               parameters

                l                                                                              o                                                                                     r
                     1                                                                               1


                                                                                                                                                                                           0.8

                                                                                                    0.5



                                                                                                                                                                                           0.7
                    0.5
                                                                                                     0
                          0        500        1000       1500                                                 0                                          500                                                        0                             500



      s                                                       t                   Number of TD-backups
                                                                                                                                     u                                                      v                                                                      w
                                                                                    (long trajectory)

                                                                             1                                                                   1                                                                  1                                                                    0.5
                                                                                                                                                                                                                                                                      Value of the cue
                                                                                                                                                                                             Value of the cue
                                                         Value of the cue




                                                                                                                          Value of the cue
                                                                                                                           (normalized)




          cue         Reward
                                                                            0.5                                                              0.5                                                                0.5

          cue                            Reward
                                                                             0                                                                   0                                                                  0                                                                     0
                                                                                                                                                     0           5            10                                            0            50         100                                        0            50        100
                                                                                  Number of TD-backups                                                    Outcome time (a.u.)                                                   Number of TD-backups                                               Number of TD-backups
                                                                                    (short trajectory)


Extended Data Fig. 1 | See next page for caption.
Article
Extended Data Fig. 1 | Decoding simulations for multi-timescale vs. single-            random rewards). l, Decoding performance as the decoder is trained. Different
timescale agents. (a-c). Experiment corresponding to Task 1 in Fig. 1. a, MDP          colors indicate the discount factors used in TD-learning. (m-o) Experiment with
with reward R at time t R . b, Diagram of the decoding experiment. In each             two rewards. m, MDP with two rewards of magnitude R1 and R2 at times t R1 < t R2 .
episode, the reward magnitude and time are randomly sampled from discrete              Value estimates Vγ(s) are fed into a non-linear decoder which learns, across MDPs,
uniform distributions, which defines the MDP in a. Values are learned until            to report both reward times. o, Decoding performance as the decoder is trained.
near convergence using TD-learning. Values with different discount factors             Different colors indicate the discount factors used in TD-learning. (p-r) Experiment
are learned independently. The learned values for the cue (s) are fed into a           to determine the shortest branch when learning from incomplete information.
non-linear decoder which learns, across MDPs, to report the reward time.               p, MDP with two possible trajectories. In this example, the upwards trajectory is
c, Decoding performance as the decoder is trained. Different colors indicate the       longer than the downwards trajectory. q, Diagram of the decoding experiment.
discount factors used in TD-learning. (d-f). Experiment corresponding to Task 2        In each episode, the length of the two branches D and the number of times that
in Fig. 1. d, MDP with reward R at time t R . e, Diagram of the decoding experiment.   TD-backups are performed for each branch are randomly sampled from uniform
In each episode, the reward magnitude and time are randomly sampled from               discrete distributions. Then, TD-backups are perform ed for the two branches
discrete uniform distributions, which defines the MDP in a. Values are learned         the corresponding number of times. After this, they are fed into a decoder which
until near convergence using TD-learning. Values with different discount               is trained, across episodes, to report the shorter branch. r, Decoding performance
factors are learned independently. The learned values for the cue (s) are fed          as the decoder is trained. Different colors indicate the discount factors used in
into a non-linear decoder which learns, across MDPs, to report the hyperbolic          TD-learning. s-w: Temporal estimates are available before convergence for
value of the cue. f, Decoding performance as the decoder is trained. Different         multi-timescale agents. s, Two experiments, one with a short wait between the
colors indicate the discount factors used in TD-learning. (g-i). Experiment            cue and reward (pink), and one with a longer wait (cyan). t, The identity of the cue
corresponding to Task 3 in Fig. 1. g, MDP with reward equal to 1 at time t R .         with the higher value for a single-timescale agent (here γ = 0.9) depends on the
h, Diagram of the decoding experiment. In each episode, the reward time and            number of times that the experiments have been experienced. When the longer
the number of TD iterations (N) are sampled from discrete uniform distributions.       trajectory has been experienced significantly more often than the short one, the
Values are learned by performing N TD-learning backups on the MDP. Values with         single-timescale agent can incorrectly believe that it has a larger value. u, For a
different discount factors are learned independently. The learned values for           multi-timescale agent, the pattern of values learned across discount factors is
the cue (s) are fed into a non-linear decoder which learns, across MDPs, to report     only affected by a multiplicative factor that depends on the learning rate, the prior
the reward time. i, Decoding performance as the decoder is trained. Different          values and the asymmetric learning experience. The pattern therefore contains
colors indicate the discount factors used in TD-learning. ( j-l). Experiment           unique information about outcome time. v,w, When plotted as a function of the
corresponding to Task 4 in Fig. 1. j, MDP with reward equal to 1 at time t R, and      number of times that trajectories are experienced, the pattern of values across
a noisy reward added to every state. k, Diagram of the decoding experiment.            discount factors is only affected by a multiplicative factor. In other words, for
In each episode, the reward time is sampled from discrete uniform distributions.       the pink cue, the larger discount factors are closer together than they are to
Values are learned by performing a single iteration of TD-learning backwards           the smaller discount factor, and the opposite for the cyan cue. This pattern is
through the MDP. Values with different discount factors are learned                    maintained at every point along the x-axis, and therefore is independent of the
independently. The learned values for the cue (s) are fed into a non-linear            asymmetric experience, and it enables a downstream system to decode reward
decoder which learns, across MDPs, to report the true value of the cue after           timing. Error bars are the standard deviations (s.d.) across 100 test episodes and
experiencing the trajectory an infinite number of times (this is, ignoring the         3 trained policy gradient (PG) networks.
Extended Data Fig. 2 | See next page for caption.
Article
Extended Data Fig. 2 | The myopic learning bias. a, Illustration of the myopic             (blue), agents with a small discount factor (myopic) are more accurate. Error
learning bias. Consider a scenario in which the upwards and downwards paths                bars are half s.d. across 10,000 episodes, maximums are highlighted with
from s and s’ are experienced only once, such that at least one path in the small          stars. e, Mean performance in this task by the agent in Fig. 1d (see main text
bifurcations is left unexplored. In state s’ (blue) the far future is more uncertain       and Methods). f, Maze to highlight the myopic learning bias in cases where
than the near future, and in state s (red) the near future is more uncertain               uncertainty arises due to incomplete exploration of the state space. Rewards
than the far future. b, When experiencing the upwards and downwards paths                  are indicated with water and fire. An example trajectory is shown with
only once, there are 4 possible scenarios depending on which path of the                   transparent arrows. The red and blue bars to the right denote the states in
corresponding bifurcations is visited. When learning from limited information,             the Lower and Upper half. g, True (grey) and estimated (green and brown)
the myopic (low γ) and farsighted (high γ) estimates would make different                  values for the example trajectory on top and shown in panel a. In the x-axis we
decisions depending on whether the V UP estimate is larger, smaller or                     highlight the starting timestep with s, the timestep when the fire is reached and
approximately equal to V DOWN (V UP ≈ V DOWN occurs when both estimates have               the timestep when the water is reached. Image of fire in panels f,g was created
similar magnitudes and some small variations in the precise magnitude of                   by dstore via SVG Repo under a CC0 1.0 licence. Images of water droplet in
rewards, the prior values or the learning parameters could change whether                  panels f,g were adapted from SVG Repo under a CC0 1.0 licence. h, Accuracy
V UP < V DOWN or V UP > V DOWN). In both s and s’, the correct decision is to follow the   (y-axis) is measured as the Kendall tau coefficient between the estimate with
upwards path (the ‘correct’ decision is the decision made by a hypothetical RL             a specific gamma (x-axis) and the true value function Vγ = 0.99. Error bars are
agent that experiences all possible trajectories an infinite number of times).             deviations across 300 sets of sampled trajectories. The red (blue) curve shows
Below we show the approximate probability that the agent chooses the correct               average accuracy for the states on the upper (lower) half of the maze, indicated
path, if it follows the myopic estimates (low γ) or far-sighted estimates (high γ).        with color lines on panel a. i, As the sampled number of trajectories increases,
Illustration of a mouse in panel a and silhouette of a raptor in panels a,b were           the myopic learning bias disappears. j, Architecture that learns about multiple
adapted from the NIAID NIH BIOART Source. Illustration of a block of cheese                timescales as auxiliary tasks. k, States are separated according to the agent
in panels a,b, was adapted from SVG Repo under a CC0 1.0 licence. c, Task                  being close to the goal (blue) or far from the goal (orange). Images in panels j,k
structure to evaluate the myopic learning bias when uncertainty arises due                 were adapted from Farama Foundation under an MIT licence. l, Accuracy of the
to stochastic rewards. The three dots collapse 5 transitions between black                 Q-values in the Lunar Lander environment as a function of their discount factor,
states. Black states give a small stochastic reward and orange states give a               estimated as the fraction of concordant state pairs between the empirical value
large deterministic reward. d, Accuracy at selecting the branch with the large             function and the discount specific Q-value estimated by the network. Error
deterministic reward under incomplete learning conditions. At state s (orange),            bars are s.e.m across 10 trained networks, maximums are highlighted with
agents with larger discount factors (far-sighted) are more accurate. At state s’           stars. See Methods for details.
    a                    20
                                                                                                                                                                        b
                                                                                                                                                                                              10
                         40                                                                                                                                                                   20


                         20                                                                                                                                                                   10
    Neurons




                                                                                                                                                                              Neurons
                         40                                                                                                                                                                   20


                         20                                                                                                                                                                   10
                         40                                                                                                                                                                   20


                         20                                                                                                                                                                   10
                         40                                                                                                                                                                   20
                                               0                                         4                                    8
                                                                                                                                                                                                           0                               4                         8
                                                         Time from reward delay cue (s)                                                                                                                              Time from reward delay cue (s)
                         c
                                          1
                                                   Exponential fit                                                                                                              f                                                          Exponential fit
                                                                                                      e                                                                                               Alpha                                    Baseline                                    Gamma
                                                                                                                                                                                                                                  15                                              1
                              Test R2




                                        0.5                                                                               1
                                                                                                                                                                                      40
                                                                                                                                                                                                                                  10
                                                                                                     Exponential R²




                                                                                                                                                                            Half 2




                                                                                                                                                                                                                         Half 2




                                                                                                                                                                                                                                                                     Half 2
                                                                                                                                                                                                                                                                                 0.5
                                          0                                                                                                                                           20
                                                                                                                                                                                                                                   5
                                                   0           0.5                   1
                                                                                                                      0.5
                                                         Train R2                                                                                                                       0                                          0                                              0
                         d                                                                                                                                                                    0      20         40                     0       5           10   15                    0     0.5      1
                                           1
                                                       Hyperbolic fit                                                                                                                                Half 1                                        Half 1                                  Half 1
                                                                                                                          0
                                                                                                                                                                                                                                                                                 40
                               Test R²




                                                                                                                              0                       0.5           1                 20
                                         0.5
                                                                                                                                      Hyperbolic R²                                                                               20


                                                                                                                                                                             Count




                                                                                                                                                                                                                         Count




                                                                                                                                                                                                                                                                         Count
                                                                                                                                                                                                                                                                                 20
                                           0

                                                   0          0.5                1
                                                                                                                                                                                        0                                          0                                              0
                                                         Train R2                                                                                                                             0        0.5           1                 0            0.5         1                     0     0.5      1
                                                                                                                                                                                              Correlations for Alpha              Correlations for Baseline                       Correlations for Gamma

    g                                                                                    Hyperbolic fit                                                                              h                                                     Simulation fit
                                                                                                                                                                                                                                                                                 40

                                                                                                                                      20                                                20
                                                                            20
        Count




                                                                    Count




                                                                                                                                  Count




                                                                                                                                                                             Count




                                                                                                                                                                                                                         Count




                                                                                                                                                                                                                                                                       Count
                                                                                                                                                                                                                                  20
                20                                                                                                                                                                                                                                                               20




                0                                                           0                                                         0                                                   0                                        0                                              0
                     0                   0.5             1                       0            0.5                1                         0                0.5     1                         0        0.5           1                 0            0.5         1                     0     0.5      1
                         Correlations for Alpha                                  Correlations for Baseline                                         Correlations for K                         Correlations for Alpha               Correlations for Baseline                      Correlations for Gamma

    i                                                                 Exponential fit: m 3044                                                                                         j                                      Exponential fit: m 3054
                                                                                                                                          20
                                                                            20
                10                                                                                                                                                                                                                                                                20
                                                                                                                                                                                        20
        Count




                                                                Count




                                                                                                                              Count




                                                                                                                                                                              Count




                                                                                                                                                                                                                           Count




                                                                                                                                                                                                                                   20                                     Count
                5



                0                                                            0                                                            0                                               0                                        0                                              0
                     0                   0.5              1                      0             0.5                    1                        0             0.5        1                     0           0.5        1                 0             0.5         1                     0     0.5      1
                     Correlations for Alpha                                          Correlations for Baseline                                 Correlations for Gamma                         Correlations for Alpha               Correlations for Baseline                      Correlations for Gamma

Extended Data Fig. 3 | See next page for caption.
Article
Extended Data Fig. 3 | Single neuron responses and robustness of fit in the               the Pearson correlations across neurons between the inferred parameter
cued delay task. a, PSTHs of single selected neurons (n = 50) responses to the            values in the two halves of the trials. Reported mean is the mean correlation
cues predicting a reward delay of 0.6 s, 1.5 s, 3.75 s, and 9.375 s (from top to          across bootstraps and reported p-value is the highest p-value for all the
bottom). Neurons are sorted by the inferred value of the discount factor γ.               bootstraps for a given parameters assessed via Student’s t-test. Distribution
Neural responses are normalized by z-scoring each neuron across its activity to           of correlations for the gain α (mean = 0.84, P < 1 × 10−20), baseline b (v, mean = 0.9,
all 4 conditions. b, PSTHs of single non-selected neurons (n = 23) responses to           P < 1.0 × 10−32) and discount factor γ (vi, mean = 0.93, P < 1.0 × 10−46). g, Same as
the cues predicting a reward delay of (from top to bottom). Neurons are sorted            panel f (lower row) but for the hyperbolic model with distribution of correlations
by the inferred value of the discount factor γ. Neural responses are normalized           for the gain α (mean = 0.86, P < 1 × 10−26), baseline b (v, mean = 0.88, P < 1.0 × 10−28)
by z-scoring each neuron across its activity to all 4 conditions. c, Variance             and shape parameter k (vi, mean = 0.76, P < 1.0 × 10 −11). h, Same as panel f (lower
explained for training vs testing data for the exponential model. For each                row) but for the exponential model simulated responses with distribution of
bootstrap, the variance explained was computed on both the half of the trials             correlations for the gain α (mean = 0.86, P < 1.0 × 10−10), baseline b (v, mean = 0.88,
used for fitting (train) and the other half of the trials (test). Neurons (n = 13) with   P < 1.0 × 10−24) and discount factor γ (vi, mean = 0.76, P < 1.0 × 10−26). Note that the
a negative variance explained on the test data are excluded from the decoding             distributions of inferred parameters are in a similar range than the fits to the data
analysis (grey dots). d, Same as panel c but for the fits for the hyperbolic model.       suggesting that trial numbers constrain the accuracy of parameter estimation.
e, Mean goodness of fit on held-out data across 100 bootstraps for each                   i, similar to the panel f (lower row) for the neurons recorded in mouse m3044,
selected neuron for the exponential and hyperbolic models. The data lies                  showing that across bootstraps, the estimates are consistent for the gain α (mean =
above the diagonal line suggesting a better fit from the exponential model as             0.64, P < 0.05 for 88/100 bootstraps, light blue P > 0.05, dark blue P < 0.05),
shown in Fig. 2f. Error bars indicate 95% confidence interval using bootstrap,            baseline b (mean = 0.86, P < 0.012) and discount factor γ (mean = 0.72, P < 0.05 for
see Methods. f, The values of the inferred parameters in the exponential model            97/100 bootstraps, light blue P > 0.05, dark blue P < 0.05). j, same as panel f (lower
are robust across bootstraps. top row, Inferred value of the parameters across            row) for the neurons recorded in mouse m3054. The estimates are consistent for
two halves of the trials (single bootstrap) for the gain α, baseline b and discount       the gain α (mean = 0.90, P < 0.0048), baseline b (mean = 0.93, P < 1.0 × 10 −5) and
factor γ, respectively. Bottom row, Distribution across n = 100 bootstraps of             discount factor γ (mean = 0.79, P < 0.0069).
Extended Data Fig. 4 | Decoding reward timing using the regularized                           P(t = 9.375 s) <1.0 × 10 −14), one-tailed Wilcoxon signed rank test, see Methods).
pseudo-inverse of the discount matrix. (a-c), Singular value decomposition                    f, The ability to decode the timing of expected future reward is not due to a
(SVD) of the discount matrix. a, left singular vectors (in the neuron space).                 general property of the discounting matrix and collapses if we randomize the
b, Singular values. The black line at 2 indicates the values of the regularization            identity of the cue responses (see Methods). g, Distribution of 1-Wassertein
term α. c, right singular vectors (in the time space). d, Decoding matrix based               distances between the reward timing and the predicted reward timing from the
on the regularized pseudo-inverse. e, Distribution of 1-wassertein distances                  decoding on the test data exponential fits (shown in Fig. 2k, top row) and on the
between the reward timing and the predicted reward timing from the decoding                   shuffled data (shown in panel f). The prediction from the test data are better
on the test data from exponential fits (shown in Fig. 2k, top row) and on the                 predictions (smaller 1-Wasserstein distance) than shuffled data (P = 1.2 × 10 −4
average exponential model (shown in Fig. 2k, bottom row). Decoding is better                  for 0.6 s reward delay, P < 1.0 × 10 −20 for the other delays, one-tailed Wilcoxon
for the exponential model from Fig. 2 than the average exponential model except               signed rank test, see Methods).
for the shortest delay (P(t = 0.6 s) = 1, P(t = 1.5 s) <1.0 × 10 −31, P(t = 3.75) = 0.0135,
Article
    a                            Lick response
                                                                 1
                                                                                   Cue response
                                                                                                                 1
                                                                 0                                               0        b                                                                                 c
                                   (normalized)                 -1
                                                                                       (normalized)              -1
                                  A         B       C       D                          A      B      C       D                                       1




                                                                                                                           Neural discounting
                                                                                                                                                                                                                   5                                                                                   Relative
                                                                                                                                                                                                                                                                                                        value




                                                                                                                                                                                                         Neurons
    m 3044




                       10                                                     10
         Neuron




                                                                     Neuron
                                                                                                                                                                                                                                                                                                           1
                                                                                                                                                    0.5                                                            10


                                                                                                                                                                                                                   15                                                                                      0
                       20                                                     20

                                                                                                                                                     0
                                  0.6      1.5 3.75 9.375                              0.6    1.5 3.75 9.375                                              0               0.5                1                          0                                    4              8                 12
                                 Reward Delay (s)                                      Reward Delay (s)                                                          Behavioral discounting                                                                          Time (s)

    d
                                                                1                                                 1
                                Lick response                   0
                                                                                   Cue response                   0       e                                                                                 f
                                  (normalized)                  -1
                                                                                           (normalized)           -1
                                 A         B        C       D                          A       B     C       D                                       1




                                                                                                                               Neural discounting
                                                                                                                                                                                                                                                                                                       Relative




                                                                                                                                                                                                         Neurons
                                                                                                                                                                                                                                                                                                        value
    m 3054
         Neuron




                                                                     Neuron




                   10                                                         10                                                                                                                                   10                                                                                       1
                                                                                                                                                    0.5


                                                                                                                                                                                                                                                                                                            0
                   20                                                         20
                                                                                                                                                                                                                   20
                                                                                                                                                     0
                                                                                                                                                             0                0.5                1                      0                                    4              8                 12
                                 0.6       1.5 3.75 9.375                              0.6    1.5 3.75 9.375
                                                                                                                                                                 Behavioral discounting                                                                          Time (s)
                                 Reward Delay (s)                                      Reward Delay (s)

    g                                                                                                                                                                                                                       h
                                                Odor A                                         Odor B                                                 Odor C                                 Odor D
                                                (0.6 s)                                        (1.5 s)                                                (3.75 s)                              (9.375 s)
                                 0.1                                                                                    0.05                                                    0.05
                                                                                                                                                                                                                                                    1
             E(reward | time)




                                                                                                                                                                                                                                High lick rate
                                                                                                                                                                                                                                                                                              5
    m 3044




                                                                              0.05




                                                                                                                                                                                                                                                                                      Count
                                0.05                                                                                   0.025                                                   0.025                                                               0.5
                                                                         0.025


                                  0                                                0                                      0                                                         0                                                               0                                         0
                                       0        4       8           12                 0       4      8      12                 0                        4          8    12             0   4        8             12                                    0            0.5         1               -1        0       1
                                                                                                                                                                                                                                                                  Low lick rate                          High-Low
                                                                                                                                                                                                                            i
                                 0.1                                                                                    0.05                                                    0.05                                                                1
             E(reward | time)




                                                                                                                                                                                                                                  High lick rate
    m 3054




                                                                              0.05



                                                                                                                                                                                                                                                                                      Count
                                0.05                                                                                   0.025                                                   0.025                                                                                                          5
                                                                                                                                                                                                                                                   0.5
                                                                         0.025


                                  0                                                0                                      0                                                         0                                                               0                                         0
                                       0        4       8           12                 0       4      8      12                0                         4          8    12             0   4        8             12                                    0            0.5         1               -1        0       1
                                                Time (s)                                          Time (s)                                            Time (s)                              Time (s)                                                              Low lick rate                         High-Low

Extended Data Fig. 5 | Comparing behavioral and neural discounting and                                                                                                    This is the matrix used for decoding in panel g, bottom row. g, Decoding of
decoding reward timing in single animals. (a-c): mouse 3044. a, left panel.                                                                                               reward timing at the single animal level for mouse 3044 (top row) and mouse
Normalized lick responses to the cues predicting reward delays across the                                                                                                 3054 (bottom row). The decoding is present but slightly less accurate as
population. For each neuron, the response was normalized to the highest                                                                                                   expected from the smaller number of neurons. h, discount factor inferred
response across the 4 possible delays. Neurons are sorted by the inferred                                                                                                 for neurons in mouse m3044 when dividing trials between low and high
behavioral discount factor. Right panel: Normalized neural responses to the                                                                                               anticipatory lick rate. left panel, scatter plot of the value across neurons.
cues predicting reward delays across the population (sorted by the behavioral                                                                                             right panel, the distribution across neurons of differences in inferred discount
discount factor). b, The behavioral and neural discount factors are not                                                                                                   across the two conditions is not significant (mean = −0.0024, P = 0.96, two-
correlated (r = −0.29, P = 0.27, Spearman’s rank correlation, two-tailed Student’s                                                                                        tailed Student’s t-test). i, Same as panel h for mouse m3054. The difference
t-test). c, Discount matrix for the neurons recorded in mouse 3044. This is the                                                                                           in inferred value between low and high lick rate is significant (mean = 0.086,
matrix used for decoding in panel g, top row. (d-f): same as panels (a-c) for                                                                                             P = 0.0062, two-tailed Student’s t-test) but the mean effect is small compared
mouse 3054. e, The behavioral and neural discount factors are not correlated in                                                                                           to the standard deviation of inferred discount factors across neurons
mouse 3054 (r = −0.029, P = 0.9, Spearman’s rank correlation, two-tailed                                                                                                  (s.d. = 0.19 for neurons in m3054).
Student’s t-test). f, Discount matrix for the neurons recorded in mouse 3054.
           a                                                                    b                                15                                                      c
                                                                                                                                                                                                                                                  1




                                                                                        K - discount parameter




                                                                                                                                                                                                                                 Relative value
                   20
                                                                                                                 10
                                                                                                                                                                              20




                                                                                                                                                                    Neurons
           Count


                   10                                                                                                                                                                                                                             0
                                                                                                                  5
                                                                                                                                                                              40


                   0                                                                                              0                                                                0               4              8         12
                        0                         5          10       15                                              0            0.5                 1
                                       K - discount parameter                                                                Discount factor                                                           Time (s)


           d                             0.1                                                                                               0.05                                         0.05
                    E(reward | time)




                                                                              0.05
                                       0.05                                                                                              0.025                                         0.025
                                                                             0.025


                                             0                                      0                                                          0                                          0
                                                 0        4      8    12                0                             4      8     12              0        4      8           12              0         4      8      12
                                                           Time (s)                                                    Time (s)                              Time (s)                                     Time (s)
           e                           200                                    200                                                                                                          40
                                                       Data - exponential
                                                                                                                                             50
                                                       Data - hyperbolic
                    Count




                                       100                                    100                                                                                                          20


                                          0                                     0                                                              0                                               0
                                              0              5                          0                              5                           0             5                                 0          5
                                                       W1 distance                                               W1 distance                               W1 distance                                  W1 distance
           f
                                        0.1                                                                                                0.05                                          0.05
                    E(reward | time)




                                                                             0.05
                                       0.05                                                                                               0.025                                         0.025
                                                                            0.025


                                         0                                     0                                                               0                                               0
                                              0          4     8      12            0                             4      8        12               0         4      8          12                  0       4      8     12
                                                          Time (s)                                                 Time (s)                                   Time (s)                                      Time (s)
           g                           100                                   200                                                                                                           50
                                                      Data - exponential
                                                                                                                                            50
                                                      Simulation
                    Count




                                        50                                   100



                                         0                                      0                                                              0                                               0
                                             0               5                      0                           5                                  0          5                                    0          5
                                                      W1 distance                                        W1 distance                                   W1 distance                                     W1 distance

Extended Data Fig. 6 | Decoding reward timing from the hyperbolic model                                                                  Decoding is better for the exponential model from Fig. 2 than the hyperbolic
and exponential model simulations. a, Distribution of the inferred discount                                                              model except for the shortest delay (P(t = 0.6 s) = 1, P(t = 1.5 s) <1.0 × 10 −31,
parameter k across the neurons. b, Correlation between the discount factor                                                               P(t = 3.75) < 1.0 × 10 −33, P(t = 9.375 s) <1.0 × 10 −3), one-tailed Wilcoxon signed rank
inferred in the exponential model of the discount parameter k from the                                                                   test, see Methods). f, Decoded subjective expected timing of future reward
hyperbolic model (r = −0.9, P < 1.0 × 10 −30, Student’s t-test). Note the in the                                                         E (r|t ) using simulated data based on the parameters of the exponential model
hyperbolic model a larger value of k implies faster discounting hence the                                                                (see Methods). g, Distribution of 1-Wassertein distances between the reward
negative correlation. c, Discount matrix for the hyperbolic model. For each                                                              timing and the predicted reward timing from the decoding on the test data
neuron we plot the relative value of future events given its inferred discount                                                           from exponential fits (shown in Fig. 2k, top row) and on the simulated data from
parameter. Neurons are sorted by decreasing estimated value of the discount                                                              the parameters of the exponential fits (shown in f). Decoding is marginally
parameter. d, Decoded subjective expected timing of future reward E (r|t ) using                                                         better for the data predictions (P(t = 0.6 s) = 0.002, P(t = 1.5 s) = 0.999,
the discount matrix from the hyperbolic model (see Methods). e, Distribution                                                             P(t = 3.75) < 1 × 10 −12, P(t = 9.375 s) = 0.027), one-tailed Wilcoxon signed rank test,
of 1-Wassertein distances between the reward timing and the predicted reward                                                             see Methods), suggesting that decoding accuracy is limited by the number of
timing from the decoding on the test data with the exponential model (shown                                                              trials.
in Fig. 2k, top row) and on the test data with the hyperbolic model (shown in d).
Article




Extended Data Fig. 7 | See next page for caption.
Extended Data Fig. 7 | Ramping, discounting, anatomy and distributed RL                mixed RL model as a function of the common value function sharing-parameter
models. a, Ramping in the prediction error signal is controlled by the relative        λ, in a linear MDP of 30 steps (x-axis in the plots) with a deterministic reward
contribution of value increases and discounting. If the value increase (middle)        equal to 1 in the last step and 0 everywhere else. Plots are shown after learning
exactly matches the discounting, there is no prediction error (middle equation,        has empirically stabilized (after 3,000 TD-learning iterations with a learning
right). If the discounting is smaller than the value increase (large discount          rate of 0.1). The dashed value function is the exponential value function without
factor) then there is a positive TD error (top equation, right). If the discounting    common value sharing (λ = 0), which would lead to a flat RPE equal to 0 at every
is larger (small discount factor) than the value increase then there a negative        state. The actual value functions (solid lines) are not purely exponential, and
TD error (bottom equation, right). A single timescale agent with no state              thus lead to ramping RPEs. f, Circuit model in which each value estimation and
uncertainty will learn an exponential value function but if there is state             their corresponding prediction error are part completely independent loops
uncertainty (see ref. 69) or the global value function arises from combining the       (λ = 0). At convergence, there is no more prediction error in the reward
contribution of single-timescale agents then the value function is likely t be         anticipation period (bottom row). g, Circuit model in which the prediction
non-exponential. Image of a magnifying glass was created by googlefonts via            error for each dopamine neuron is influenced by both the independent value
SVG Repo under an Apache Licence. b, Intuition for diversity of ramping with           signal and the shared a common value signal (C, λ = 0.1). The dashed line
a hyperbolic value function. Agents with a small discount factor exhibit a             indicate the value function corresponding to completely separate loops, and
monotonic downward ramp (pink), while those with a large discount factor               the solid function the actual value function due to the influence of the common
exhibit a monotonic upward ramp (brown). Agents with an intermediary                   value signal. The difference between them leads to ramping in the reward
discount factor tend to exhibit a downward then upward ramp. The hyperbolic            prediction error signals (bottom row). h, Circuit model with a strong influence
value function gets increasingly convex as the reward approaches, so an                of the common value signal (λ = 1) which also leads to ramping in the reward
increasing fraction of the agents have a positive prediction error as they             prediction error signals. See Methods for details. i, Decoded reward times for
approach the reward. c, The discount factor inferred in the VR task is not             4 experimental conditions with rewards at times 5, 10, 15 and 30 (pink to cyan),
correlated with the medio-lateral (ML) position of the implant (Pearson’s              by applying a regularized inverse Laplace decoder (analogous to the one used
r = 0.015, P = 0.89, two-tailed Student’s t-test). d, The baseline parameter           in Fig. 2 of the main text) to the values at the moment of the cue, under the
inferred in the VR task is not correlated with the medio-lateral (ML) position         model without mixing λ = 0. j, Same as (i) but using a mixing factor of λ = 0.1.
of the implant (Pearson’s r = −0.011, P = 0.92, two-tailed Student’s t-test). e, The   The small mixing factor does not affect the quality of the temporal decoding,
inferred gain in the VR task reduces with increasing medio-lateral (ML) position       while creating a ramping reward prediction error (panel g). Therefore, a small
but the effect does not reach significance (Pearson’s r = −0.19, P = 0.069, two-       mixing factor constitutes a common model that can qualitatively account for
tailed Student’s t-test). In panels c-e, the line correspond to the best fit linear    the two tasks studied in the paper. k, Same as (i) but using a mixing factor of
regression and the uncertainty shading represents 95% confidence interval on           λ = 1. Using a fully shared value function the relative differences between
a linear regression fit. f-h. Ramping in the reward prediction error with mixing       discount factors disappear, so temporal decoding is no longer possible.
in distributed RL models. Inferred value functions (V) and RPEs (δ) for the
Article
     a                                                                                               b                                 1




                                                                                                           Slope in spiking activity
               Lick rate        10



                                                                                                                                       0
                                    5




                                    0                                                                                                  -1
                                                 -4                           0                                                             0                              0.5                   1
                                         Time to reward (s)                                                                             Neuronal discount factor

                                                                                                     m 3044                                                                                                                                                                                                         m 3054
     c                                   early-late                                                  middle-late                                                                         early-middle            d                                   early-late                                                      middle-late                                              early-middle
                  i                                                               ii                                                                               iii                                                      i                                                             ii                                                          iii 1
                                1                                                            1                                                                                 1                                                            1                                                               1
          Ramp in licking




                                                                       Ramp in licking




                                                                                                                                                        Ramp in licking




                                                                                                                                                                                                                      Ramp in licking




                                                                                                                                                                                                                                                                                      Ramp in licking




                                                                                                                                                                                                                                                                                                                                             Ramp in licking
                            0.5                                                          0.5                                                                               0.5                                                          0.5                                                             0.5                                                     0.5


                                0                                                            0                                                                                 0                                                            0                                                               0                                                       0


                            -0.5                                                         -0.5                                                                             -0.5                                                          -0.5                                                       -0.5                                                         -0.5
                                    0          0.5          1                                    0          0.5                                 1                                  0           0.5         1                                    0         0.5         1                                         0         0.5         1                                 0           0.5         1
                                    Neural discount factor                                       Neural discount factor                                                            Neural discount factor                                   Neural discount factor                                              Neural discount factor                              Neural discount factor
                   iv 1                                                        v             1                                                                       vi 1                                                    iv             1                                          v                    1                                           vi 1
          middle-late




                                                                                                                                                                                                                      middle-late




                                                                                                                                                                                                                                                                                    early-middle




                                                                                                                                                                                                                                                                                                                                                 early-middle
                                                                        early-middle




                                                                                                                                                         early-middle




                            0.5                                                          0.5                                                                               0.5                                                          0.5                                                             0.5                                                     0.5


                                0                                                            0                                                                                 0                                                            0                                                               0                                                       0


                            -0.5                                                         -0.5                                                                             -0.5                                                          -0.5                                                       -0.5                                                         -0.5
                               -0.5       0          0.5    1                               -0.5       0                         0.5            1                            -0.5          0         0.5   1                               -0.5       0         0.5   1                               -0.5            0         0.5   1                            -0.5         0         0.5   1
                                          early-late                                                  early-late                                                                          middle-late                                                early-late                                                      early-late                                               middle-late

     e                                  Data               Normalized                                 Model fit                                              f                                                       g                              Data               Normalized                                     Model fit                                h
                                                            spike rate                                                                                                                                                                                                  spike rate
                                                                 Max                                                                                                                                                                                                            Max

                                                                                                                                                                          1                                                                                                                                                                               1
                                                                 0                                                                                                                                                                                                              0
                                                                                                                                                                                                                           20                                                                           20




                                                                                                                                                                                                                                                                                                                                               Inferred value
                                                                                                                                                          Inferred value




          20                                                     - Max                                                                                                                                                                                                          - Max
   Neurons




                                                                                    20
                                                                                                                                                                                                                 Neurons




          10                                                                                                                                                                                                               10                                                                           10
                                                                                    10



                                                                                                                                                                          0                                                                                                                                                                               0
                        -4      -2       0                                                    -4      -2       0                                                               -4              -2         0                              -4       -2     0                                                      -4       -2      0                                  -4            -2         0
                       Time from reward (s)                                                  Time from reward (s)                                                                      Time from reward (s)                               Time from reward (s)                                                  Time from reward (s)                                     Time from reward (s)


     i                                    Gain                                                       Baseline                                                                          Discount factor                   j                           Gain                                                           Baseline                                            Discount factor
              40                                                            10                                                                                    1                                                        50                                                         10                                                                  1
  m3044 fit




                                                                m3044 fit




                                                                                                                                                     m3044 fit




                                                                                                                                                                                                               m3054 fit




                                                                                                                                                                                                                                                                          m3054 fit




                                                                                                                                                                                                                                                                                                                                           m3054 fit
              20                                                                5                                                                                                                                          25                                                                5



                  0                                                             0                                                                                 0                                                             0                                                            0                                                            0
                            0              20              40                            0                 5                                    10                         0                                                            0              25             50                                0                 5           10                        0                               1
                                    population fit                                               population fit                                                                        population fit                                           population fit                                                  population fit                                              population fit

Extended Data Fig. 8 | See next page for caption.
Extended Data Fig. 8 | Behavioral and neural discounting at the single                compute ramping activity in the lick rate: i, modulation between the late and
animal. a, Time course of the lick rate in the VR task as mice approach the           early window, r = −0.1, P = 0.62. ii, modulation between the late and middle
reward location. gray line, lick rate for individual neurons, blue line, mean lick    windows, r = 0.032, P = 0.88. iii, modulation between the early and middle
rate. The three black lines on top indicate the three windows used to compute         windows, r = −0.11, P = 0.61. iv-vi: The measures of ramping in licking activity
early, middle and late lick rate in the analysis presented in panels c-d. b, The      are strongly correlated to each other: i, correlation between the late-middle
inferred discount factor and the slope in spiking activity (see Fig. 3b) are          and late-early modulation measures, r = 0.82, P = 6.7 × 10 −7. ii, correlation
strongly correlated (r = 0.81, P = 0, Spearman rank correlation, two-tailed           between the middle-early and late-early modulation measures, r = 0.94,
Student’s t-test) suggesting that slope is a good measure of discounting.             P = 4.7 × 10 −12 . iii, correlation between the middle-early and late-middle
c, Correlations of measures of behavioral and neural discounting for mouse            modulation measures, r = 0.66, P = 4.2 × 10 −4. e, The VR model fits (right panel)
m3044 (Spearman rank correlation, two-tailed Student’s t-test). i-iii: the neural     to m3044 neurons alone captures the diversity of ramping activity observed
discount factor and the ramp in licking activity is not correlated irrespective       across single neurons (left panel). f, Inferred value function for m3044. Thin
of the window used to compute the ramp in licking activity when using the             gray line, individual bootstrap fits. Blue line, mean value fit. g, The VR model
following windows to compute ramping activity in the lick rate: i, modulation         fits (right panel) to m3054 neurons alone captures the diversity of ramping
between the late and early window, r = 0.09, P = 0.64. ii, modulation between         activity observed across single neurons (left panel). h, Inferred value function
the late and middle windows, r = −0.05, P = 0.79. iii, modulation between the         for m3054. Thin gray line, individual bootstrap fits. Blue line, mean value fit.
early and middle windows, r = 0.17, P = 0.37. iv-vi: The measures of ramping in       i, The inferred parameter values between the fit for m3044 and the full
licking activity are strongly correlated to each other: i, correlation between        population fit are strongly correlated (Spearman rank correlation, two-
the late-middle and late-early modulation measures, r = 0.65, P = 1.3 × 10 −4.        tailed Student’s t-test) for the gain parameter (left panel, r = 0.78, P = 2.1 × 10 −6),
ii, correlation between the middle-early and late-early modulation measures,          the baseline parameter (middle panel, r = 0.75, P = 5.7 × 10 −6) and the discount
r = 0.96, P = 7 × 10 −17. iii, correlation between the middle-early and late-middle   factor (right panel, r = 0.75, P = 6.8 × 10 −6). j, The inferred parameter values
modulation measures, r = 0.53, P = 0.0028. d, Correlations of measures                between the fit for m3054 and the full population fit are strongly correlated
of behavioral and neural discounting for mouse m3054 (Spearman rank                   (Spearman rank correlation, two-tailed Student’s t-test) for the gain parameter
correlation, two-tailed Student’s t-test). i-iii: the neural discount factor and      (left panel, r = 0.97, P = 1.3 × 10 −6), the baseline parameter (middle panel, r = 0.98,
the ramp in licking activity is not correlated irrespective of the window used        P = 1.1 × 10 −6) and the discount factor (right panel, r = 0.88, P = 2.6 × 10 −6).
to compute the ramp in licking activity when using the following windows to           All reported correlations are Spearman rank correlations.
Article
                            a                  Uncertainty in time to reward                                                                                                                                                                                                                                                                                   Time




                                                                   0.2                                                                                                                                                                                                                              1


                                                                                                                                                                                                   0.2

                                                  P( E(r|Т) )




                                                                                                                                                                                     P( E(r|t) )




                                                                                                                                                                                                                                                                                   P( E(r|t) )
                                                                   0.1                                                                                                                                                                                                                           0.5
                                                                                                                                                                                                   0.1


                                                                    0                                                                                                                               0                                                                                               0
                                                                         0               4                       8                                                                                       0                 4                       8                                                        0                         4                    8
                                                                         Expected time to reward (s)                                                                                                     Expected time to reward (s)                                                                        Expected time to reward (s)



                                                                                                                                                                                             Simulation (Multiple value model)
                            b                                                                     Common to all neurons                                                                                                                                                                Distinct for each neuron

                                                              Uncertainty in time to reward                                                                  Expected time to reward                                                                                      Value                                                                                    TD error




                                                                                                                                                  -5                                                                                                  1                                                                                      1
                                expected time to reward




                                                                                                                             Time to reward (s)




                                                          4
                                 Standard deviation of




                                                                                                                                                                                                                                        Discount factor




                                                                                                                                                                                                                                                                                                                           Discount factor
                                                          2



                                                          0                                                                                        0                                                                                             0.5                                                                                   0.5
                                                                         -4              -2                       0                                    0                                           5                       10                                  -4                 -2                            0                                     -4                    -2                         0
                                                                         Time to reward (s)                                      Subjective expected time to reward (s)                                                                                        Time to reward (s)                                                                     Time to reward (s)


                                                                                                                                                                                                                Fit to the data
                            c                                                                                                                          d                                                                                                       e
                                       of expected reward timing




                                                                                                                                                                                                                                                                         80
                                                                    2                                                                                                       -4
                                          Standard deviation




                                                                                                                                                       Time to reward (s)




                                                                                                                                                                                                                                                                                                                                                                                    10
                                                                                                                                                                                                                                                                Neuron




                                                                                                                                                                                                                                                          1




                                                                                                                                                                                                                                                                                                                                                                    Value
                                                                                                                                                                                                                                            P(r|t)




                                                                    1                                                                                                       -2                                                                                           40
                                                                                                                                                                                                                                                          0                                                                                                                             0



                                                                    0                                                                                                       0                                                                                             1
                                                                             -4                   -2                                      0                                      0                             4                          8                                       -4                                  -2                                       0
                                                                                    Time to reward (s)                                                            Subjective expected time to reward (s)                                                                                         Time to reward (s)


                                f                                                                      g                                                                                                       h                              Model fit                                      i                                       Data

                                                                                                                                                                                                                      80                                                                            80
                                                   40                                                                       1
                                                                                                       (Multiple values)
                                                                                                        Discount factor




                                                                                                                                                                                                                                                                                                                                                                            Normlaized response




                                                                                                                                                                                                                                                                                                                                                                                                  1
                                                                                                                                                                                                             Neuron




                                                                                                                                                                                                                                                                                           Neuron
                                    Count




                                                   20                                                                      0.5
                                                                                                                                                                                ρ=0.93                                40                                                                            40                                                                                            0
                                                                                                                                                                                p<10-20                                                                                                                                                                                                           -1
                                                      0                                                                     0
                                                            0                     0.5         1                                  0                                     0.5                         1
                                                                   Discount factor                                                                 Discount factor
                                                                                                                                                                                                                      1                                                                                 1
                                                                                                                                                  (Common value)                                                               -4                         -2                  0                                 -4                               -2                    0
                                                                                                                                                                                                                                    Time to reward (s)                                                               Time to reward (s)


Extended Data Fig. 9 | Discounting heterogeneity explains ramping                                                                                                                                                                   the reward expectation model reduces as a function of time to reward. Line
diversity in a common reward expectation model. a, Uncertainty in reward                                                                                                                                                            indicates the mean inferred standard deviation and the shading indicates the
timing reduces as mice approach the reward zone. Not only does the mean                                                                                                                                                             standard error of the mean over 100 bootstraps. d, Expected timing of the
expected reward time reduces but the standard deviation of the estimate also                                                                                                                                                        reward as a function of true time to reward. As the mice approach the reward
reduces. Distribution in the bottom row from fitted data (see panels c-e).                                                                                                                                                          not only does the mean expected time to reward reduces but the uncertainty of
b, A model where each neuron contributes to its individual value function but                                                                                                                                                       the reward timing captured by the standard deviation shown in c also reduces.
share a common reward expectation predicts ramping heterogeneity across                                                                                                                                                             This effect leads to increasingly convex value functions that lead to the
neurons. Left panel, as mice approach reward, the uncertainty, quantified                                                                                                                                                           observed ramps in dopamine neuron activity. e, Value function for each
by the standard deviation, of reward timing reduces. 2nd panel from left, The                                                                                                                                                       individual neuron (same order as in h-i). f, Distribution of inferred discount
Expectation of reward timing takes the form of a folded normal distribution.                                                                                                                                                        factors under the common reward expectation model. g, Although the range of
As the mice approach the reward there is a reduction of both the mean and the                                                                                                                                                       discount factor between the fits from the common value (x axis) and common
standard deviation of the expected reward timing distribution. 3rd panel from                                                                                                                                                       reward expectation (y axis) models differs, the inferred discount factors are
left, each neuron computes a distinct value function given their individual                                                                                                                                                         strongly correlated for single neurons (Spearman’s ρ = 0.93, P < 1.0 × 10 −20, two-
discount factor and the common expected reward timing distribution with.                                                                                                                                                            tailed Student’s t-test). h, Predicted ramping activity from the model fits under
Right panel, The diverse value functions across neurons lead to ramping                                                                                                                                                             the common reward expectation model. i, Diversity of ramping activity across
heterogeneity across neurons in the reward prediction error. (see Methods                                                                                                                                                           single neurons as mice approach reward (aligned by inferred discount factor in
‘Common Reward Expectation model’). c, The inferred standard deviation of                                                                                                                                                           the common reward expectation model).
Extended Data Fig. 10 | Decoding reward timing in the cued delayed reward         identities of the cue responses. d, Except for the shortest delay, decoded
task using parameters inferred in the VR task and details of recordings.          reward timing is more accurate than shuffle as measured by the 1-Wassertsein
a, Discount matrix computed using the parameters inferred in the VR tasks         distance (Pt = 0.6s = 1, Pt = 1.5s < 1.1 × 10 −20, Pt = 3.75s < 3.8 × 10 −20, Pt = 9.375s < 2.9 × 10 −5).
for neurons recorded across both tasks and used in the cross-task decoding.       e, Breakdown of the number of recorded neurons per animal and task. The
b, Dopamine neurons cue responses in the cued delay task. Neurons are aligned     numbers in parenthesis indicate the number of neurons included in the analysis.
as in a according to increasing discount factor inferred in the VR task. c, Top   ± indicates standard deviation across sessions. The maximum number of
row: Decoded reward timing using discount factors inferred in the VR task.        neurons recorded in a single session was 4 in both the cued delay task and the
Bottom row: The ability to decode reward timing is lost when shuffling the        VR task.
